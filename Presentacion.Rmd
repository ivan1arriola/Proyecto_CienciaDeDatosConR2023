---
title: "Datos de Transito"
subtitle : "Ciencia de datos con R"
author: "Iván Arriola, Federico Miquelerena, Damián Rovetta"
date: "12-07-2023"
output:
  beamer_presentation:
    theme: "default"
    colortheme: "beaver"
    fonttheme: "professionalfonts"
fontsize : "9pt"
header-includes:
  \usepackage{caption}
  \captionsetup[figure]{font=scriptsize}
  \renewcommand{\figurename}{Figura}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

library(tidyverse)
library(DBI)
library(RPostgres)
library(sf)
library(paletteer)
library(geouy)
library(spdep)
library(rpart)
library(rpart.plot)
source(here::here("utils.R"))
library(randomForest)
library(modelr)
```

## Introducción

Los datos utilizados en este proyecto fueron sacados de Catalogo de Datos Abiertos de **gub.uy**.
  Los mismos corresponden a los datos de transito de la ciudad de Montevideo en el periodo de Enero 2021 a Mayo 2023.
  En particular, los datos elegidos son los siguientes:

  - [Conteo vehicular en las principales avenidas de Montevideo](https://catalogodatos.gub.uy/dataset/intendencia-montevideo-conteo-de-vehiculos-del-centro-de-gestion-de-la-movilidad)
  - [Velocidad promedio vehicular en las principales avenidas de Montevideo](https://catalogodatos.gub.uy/dataset/intendencia-montevideo-velocidad-promedio-vehicular-en-las-principales-avenidas-de-montevideo)
  - [Ubicación de sensores de medición de conteo vehículos](https://catalogodatos.gub.uy/dataset/intendencia-montevideo-ubicacion-de-sensores-de-medicion-de-conteo-vehiculos)

  Los tres dataset son mantenidos por la Intendencia de Montevideo.


## Base de datos para el proyecto

Para el proyecto se utilizo una base de datos que contiene los tres dataset mencionados anteriormente.

Se combinaron los datos del dataset dando origen a 3 tablas:

### Tabla: fct_registros

- Cantidad de datos: 85386695.
- Variables de la tabla:
  - `id_registros`: _Numérico_ (_Primary Key_).
  - `id_carril`: _Numérico_.
  - `id_fecha`: _Numérico_ (_Foreign Key_, vinculado con `d_sensores`). La fecha de la que fue tomada el registro, tiene el formato _YYYY-MM-DD_
  - `id_hora`: _Numérico_. Hora en la que fue tomado el registro con formato _HHMM_.
  - `id_detector`: _Numérico_ (_Foreign Key_, cinculado con `d_date`).
  - `volume`: _Numérico_. Cantidad de vehiculos que pasaron en los últimos 5 minutos.
  - `volumen_hora`: _Numérico_. Cantidad de vehiculos que pasaron en la ultima hora.
  - `velocidad`: _Numérico_. Velocidad promedio de los vehiculos registrados en los utimos 5 minutos. Unidad en km/h
  
## Base de datos para el proyecto

### Tabla: d_sensores

- Cantidad de datos: 273
- Variables de la tabla:
  - `id_detector`: _Numérico_ (_Primary Key_).
  - `dsc_avenida`: _Texto_. Calle donde se encuentra el sensor.
  - `dsc_int_anterior`: _Texto_. Cruce previo de la calle en `dsc_avenida`.
  - `dsc_int_siguiente`: _Texto_. Cruce posterior de la calle en `dsc_avenida`. Estas dos juntas nos dirá que cada sensor se encuentra en _Avenida_ entre _Anterior_ y _Siguiente_.
  - `latitud`: _Numérico continuo_.
  - `longitud`: _Numérico continuo_. Junto a `latitud` nos indica las coordenadas geograficas del sensor.
  - `barrio`: _Texto_. Esta variable fue creada a partir del paquete `geouy`
  
Y una tercer tabla que contiene los datos de las fechas de los registros.  

## Base de datos para el proyecto

### Tabla: d_date

- Cantidad de datos: 3652
- Agunas de las variables de esta tabla son:
  - `id_fecha`: _Numérico_ (_Primary Key_)
  - `date_actual`: _Fecha_. Secuencia de fechas desde el 01-01-2021 con formato _YYYY-MM-DD_
  - `day_of_week`: _Numérico_. Dia de la semana que indica 1 como lunes, 2 como martes, etc.
  - `day_of_month`: _Numérico_. Fecha del mes, va desde 1 hasta 31.
  - `day_of_quarter`: _Numérico_. Dia del cuatrimestre.
  - `day_of_year`: _Numérico_. Dia del año, del 1 al 366.
  - `month_actual`: _Numérico_. Mes del año tomado como numero, enero como 1, febrero como 2 y así sucesivamente.
  - `year_actual`: _Numérico_. Indica el año.
  - `mmyyyy`: _Numérico_. Secuencia de caracteres que indica el mes y el año en formato MMYYY
  - `mmddyyyy`: _Numérico_. Secuencia de caracteres que indica el mes, la fecha y el año en formato MMDDYYY.
  - `weekend_indr`: _Lógico_. `TRUE` si la fecha tiene como dia de la semana sabado o domingo, `FALSE` en caso contrario.
  - `feriado`: _Lógico_. `TRUE` si la fecha correspondiente coincide con dias feriados en Uruguay, `FALSE` en caso contrario.




## Datos faltantes y nulos

```{r Cantidad de datos faltantes y nulos, echo = FALSE, results='asis'}
options(xtable.comment = FALSE)
# Cantidad de datos faltantes y nulos
tabla_datos_null <- load_data('tabla_datos_null.csv',con,
"
SELECT 'velocidad' AS atributo,
       COUNT(*) AS cant_total,
       SUM(CASE WHEN velocidad IS NULL THEN 1 ELSE 0 END) AS cant_null,
       SUM(CASE WHEN velocidad = 0 THEN 1 ELSE 0 END) AS cant_0
FROM fct_registros
UNION ALL
SELECT 'volumen_hora' AS atributo,
       COUNT(*) AS cant_total,
       SUM(CASE WHEN volumen_hora IS NULL THEN 1 ELSE 0 END) AS cant_null,
       SUM(CASE WHEN volumen_hora = 0 THEN 1 ELSE 0 END) AS cant_0
FROM fct_registros
UNION ALL
SELECT 'volume' AS atributo,
       COUNT(*) AS cant_total,
       SUM(CASE WHEN volume IS NULL THEN 1 ELSE 0 END) AS cant_null,
       SUM(CASE WHEN volume = 0 THEN 1 ELSE 0 END) AS cant_0
FROM fct_registros
UNION ALL
SELECT 'id_fecha' AS atributo,
       COUNT(*) AS cant_total,
       SUM(CASE WHEN id_fecha IS NULL THEN 1 ELSE 0 END) AS cant_null,
       SUM(CASE WHEN id_fecha = 0 THEN 1 ELSE 0 END) AS cant_0
FROM fct_registros
UNION ALL
SELECT 'id_detector' AS atributo,
       COUNT(*) AS cant_total,
       SUM(CASE WHEN id_detector IS NULL THEN 1 ELSE 0 END) AS cant_null,
       0 AS cant_0
FROM fct_registros;
") %>%
  mutate(
    porc_null = round(cant_null / cant_total * 100, 6),
    porc_0 = round(cant_0 / cant_total * 100, 6)
  )
print(xtable::xtable(tabla_datos_null))
```

En `r tabla_datos_null[5,3]` datos se perdio la informacion de la ubicacion del sensor, por lo que no sabemos de que calle se trata.
En el `r round(tabla_datos_null[1,6], 2)`% de los datos se detecto velocidad 0 y en el mismo porcentaje se detecto volumen 0.
Se descubrió que la cantidad de datos que tienen los campos de `velocidad` , `volume` y `volumen_hora` igual a 0 es `r tabla_datos_null[1,4]` registros, lo que representa el `r round(tabla_datos_null[1,6], 2)`% de los datos nulos.


```{r, echo = FALSE}
# Cantidad de datos que tienen los tres campos en 0 y no tienen id_detector
datos_0_null <- load_data('datos_0_null.csv',con,
"
SELECT
    COUNT(CASE WHEN fct_registros.velocidad = 0 AND fct_registros.volume = 0 AND fct_registros.volumen_hora = 0 AND fct_registros.id_detector IS NULL THEN 1 END) AS registros_cero_null
FROM
    fct_registros
")

# Cantidad de datos que tienen los tres campos en 0 y tienen id_detector
datos_0_not_null <- load_data('datos_0_not_null.csv',con,
"
SELECT
    COUNT(CASE WHEN fct_registros.velocidad = 0 AND fct_registros.volume = 0 AND fct_registros.volumen_hora = 0 AND fct_registros.id_detector IS NOT NULL THEN 1 END) AS registros_cero_not_null
FROM
    fct_registros
")
```

De aquellos `r tabla_datos_null[1,4]` registros de valor cero `r datos_0_null[1,1]` no tienen id_detector.

No queda claro si los datos que tienen los tres campos en 0 son datos que representan que no paso ningun vehiculo por el sensor o si son datos que no se pudieron obtener.
  

## Datos Faltantes o Nulos

Sobre los datos faltantes de la ubicación del sensor, son datos que no se pueden recuperar, por lo que se tendrán que descartar.

Se quiso averiguar en que fecha se perdió la información de la ubicación del sensor, para ver si se podía recuperar la informacion de otra forma, pero no se pudo.

```{r, echo = FALSE, results='asis'}
options(xtable.comment = FALSE)
# Fecha en la que se perdio la informacion de la ubicacion del sensor
fecha_null <- load_data('fecha_null.csv',con,
"
SELECT id_fecha,
       COUNT(*) AS cant_total,
       SUM(CASE WHEN id_detector IS NULL THEN 1 ELSE 0 END) AS cant_null
FROM fct_registros
GROUP BY id_fecha
HAVING SUM(CASE WHEN id_detector IS NULL THEN 1 ELSE 0 END) > 0
ORDER BY id_fecha;
") %>%
  mutate(
    porc_null = round(cant_null / cant_total * 100, 6)
  )

print(xtable::xtable(fecha_null))
```
## Datos Faltantes o Nulos
```{r, echo = FALSE}

datos_null <- load_data('datos_null.csv',
  con,
  "SELECT *
  FROM fct_registros
  WHERE id_detector IS NULL
  "
)

datos_null$fecha <- as.Date(as.character(datos_null$id_fecha), format = "%Y%m%d")
datos_null$hora <- datos_null$id_hora

summary(datos_null)
```


## Datos Faltantes o Nulos

Se puede ver que los datos faltantes de la ubicación del sensor van desde el 24/07/2021 hasta el 31/07/2021.
Cambien se puede ver que todos los datos faltantes son del carril 2, quiza se podrían recuperar los datos de la ubicación del sensor revisando los datos originales pero es irrelevante
ya que son pocos datos y no afectan al análisis.
```{r Meses presentes en la base de datos, echo=FALSE}
  registros_año_mes <- load_data("registros_año_mes.csv",
    con,
    "
       SELECT
        count(*),
    d_date.month_actual, d_date.year_actual
    FROM fct_registros
    LEFT JOIN d_date ON fct_registros.id_fecha = d_date.id_fecha
    GROUP BY d_date.month_actual, d_date.year_actual

    "
  )
```
Teniendo en cuenta que los datos van desde enero 2021 a mayo 2023 (28 meses) nos hemos dado cuenta que faltan algunos meses en la base de datos, en particular `r 28-nrow(registros_año_mes)` meses


## Ubicacion de los sensores
Los datos que tenemos son de 100 sensores ubicados todos en Montevideo y estos van cambiando de ubicacion cada mes.
Por lo que el primer paso es ver cuantas ubicaciones distintas tenemos y cuantos sensores hay en cada ubicacion.
```{r Cargar mapas y sensores,  message=FALSE, results='hide'}
d_sensores <- load_data("d_sensores.csv",
  con,
  "SELECT * FROM d_sensores"
  )
mvd_map <- load_geouy("Barrios")
mvd_map_fixed <- st_make_valid(st_transform(mvd_map, crs = 4326))
puntos_sensores <- d_sensores %>%
  select(barrio, latitud, longitud) %>%
  mutate(transformarCoord(latitud, longitud, mvd_map))
```

## Ubicacion de los sensores
```{r Cantidad de sensores por barrio, fig.cap = 'Mapa de Montevideo con cantidad de sensores por barrio.'}
cant_sensores <- d_sensores %>%
  group_by(barrio) %>%
  summarise(
    cant_de_sensores = n()
    ) %>%
  arrange(desc(cant_de_sensores))

mvd_map_sensores <- mvd_map_fixed %>%
  left_join(cant_sensores, by = c("nombbarr" = "barrio")) %>%
  mutate(cant_de_sensores = ifelse(is.na(cant_de_sensores), 0, cant_de_sensores))

ggplot() +
  geom_sf(
    data = mvd_map_sensores,
    aes(
      fill = ifelse(cant_de_sensores == 0, NA, cant_de_sensores)
    )
  ) +
  scale_fill_viridis_c(option = "plasma", name = "Cantidad de sensores por barrio") +
  theme_void() +
  theme(legend.position = "bottom")
```

## Ubicacion de los sensores
Ahora quiero mostrar la cantidad de datos que tenemos por ubicación, para ver si hay alguna ubicación en particular que
tenga mas o menos datos que las demás.
```{r Cantidad de datos por ubicacion, echo = FALSE}
cant_datos_por_ubicacion <- load_data("cant_datos_por_ubicacion.csv",
  con,
  "
    SELECT
      id_detector,
      d_date.mmyyyy,
      COUNT(*) AS cant_datos
    FROM fct_registros
    LEFT JOIN d_date on d_date.id_fecha = fct_registros.id_fecha
    GROUP BY id_detector, d_date.mmyyyy
  "
) %>% inner_join(
  d_sensores %>% select(barrio, id_detector), by = c("id_detector" = "id_detector")
) %>% inner_join(
  cant_sensores, by = c("barrio" = "barrio")
)
```
```{r Cantidad de datos por ubicacion 2, fig.cap = 'Cantidad de datos por ubicacion'}
cant_datos_por_barrio <- cant_datos_por_ubicacion %>%
  group_by(barrio) %>%
  summarise(
    cant_datos = sum(cant_datos)
  ) %>%
  arrange(desc(cant_datos))
```
```{r Cantidad de datos por ubicacion 3, fig.cap = 'Cantidad de datos por ubicacion'}
cant_datos_por_barrio %>%
  ggplot(aes(x = fct_reorder(barrio, cant_datos), y = cant_datos, fill = barrio)) +
  geom_bar(stat = "identity", position = "stack") +
  theme(axis.text.x = element_text(size= 5, angle = 45, hjust = 1)) +
  scale_x_discrete(name = "Ubicacion") +
  scale_y_continuous(labels = scales::comma, name = "Cantidad de datos") +
  theme(
    legend.position = 'none'
  )

```

## Ubicacion de los sensores
Se puede observar que la cantidad de datos por ubicación no es para nada homogénea.
Los barrios con mayor cantidad de datos aportados al dataset son Unión, Pocitos, Tres Cruces, Buceo y Centro.
Por otro lado Maroñas, Carrasco, Las Canteras y Cerrito son los que menos datos aportan.

## Ubicacion de los sensores
Ahora me interesaría saber cuales son los barrios mejores representados en el dataset, es decir, cuales son los barrios que tienen mas datos por metro cuadrado. Primero calculo el area de cada barrio y luego calculo la cantidad de datos por metro cuadrado.
```{r Mapa Montevideo con Area, echo = FALSE}
mvd_map_con_area <- mvd_map_fixed %>%
  mutate(
    area = as.numeric(st_area(mvd_map_fixed))
  ) %>% arrange(area)

```
```{r Cantidad de datos por ubicacion ponderado por area}
cant_datos_por_barrio_area <- cant_datos_por_ubicacion %>%
  group_by(barrio) %>%
  summarise(
    cant_datos = sum(cant_datos)
  ) %>%
  inner_join(
    mvd_map_con_area %>% select(nombbarr, area), by = c("barrio" = "nombbarr")
  ) %>%
  mutate(
    cant_datos_por_area = cant_datos / area
  ) %>%
  arrange(desc(cant_datos_por_area))
```
```{r Grafico Cantidad de datos por ubicacion ponderado por area, echo = FALSE, fig.cap = 'Cantidad de datos por ubicacion ponderado por area'}
cant_datos_por_barrio_area %>%
  ggplot(aes(x = fct_reorder(barrio, cant_datos_por_area), y = cant_datos_por_area, fill = barrio)) +
  geom_bar(stat = "identity", position = "stack") +
  theme(axis.text.x = element_text(size= 5, angle = 45, hjust = 1)) +
  scale_x_discrete(name = "Ubicacion") +
  scale_y_continuous(labels = scales::comma, name = "Cantidad de datos por metro cuadrado") +
  theme(
    aspect.ratio = 0.5,
    legend.position = 'none'
  )

```

## Ubicacion de los sensores
Se puede observar que los barrios con mayor cantidad de datos por metro cuadrado son Pocitos,
La Blanqueada, Jancito Vera, Centro y Tres Cruces.
Por otro lado Maroñas, Carrasco, Las Canteras, Peñarol y Cerrito son los que menos datos aportan por metro cuadrado.

## Principales variables
Las variables que se van a analizar son las siguientes:

- `volume`: _Numérico_. Cantidad de vehiculos que pasaron en los últimos 5 minutos.
- `volumen_hora`: _Numérico_. Cantidad de vehiculos que pasaron en la ultima hora.
- `velocidad`: _Numérico_. Velocidad promedio de los vehiculos registrados en los utimos 5 minutos. Unidad en km/h

### Velocidad
Resumen de la variable velocidad
```{r Resumen de la variable velocidad, echo = FALSE, results='asis'}
options(xtable.comment = FALSE)
    resumen_velocidad <- load_data("resumen_velocidad.csv",
    con,
      "
        SELECT
          MIN(velocidad) AS minimo,
          PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY velocidad) AS 1err_cuartil,
          PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY velocidad) AS mediana,
          PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY velocidad) AS 3er_cuartil,
          MAX(velocidad) AS max,
          AVG(velocidad) AS prom,
          STDDEV(velocidad) AS desvio
        FROM fct_registros
      "
    )
print(xtable::xtable(resumen_velocidad))
```
Ahora veamos la distribucion de la velocidad registrada
Para mejor visualizacion, voy a dejar de lado los datos donde la velocidad es 0


## Principales variables: Velocidad

```{r Datos de velocidad y Volumen, echo = FALSE}
  velocidad_volumen <- load_data('velocidad_volumen.csv',
  con,
  "
  SELECT
    fct_registros.velocidad,
    fct_registros.volume AS volumen
  FROM
    fct_registros TABLESAMPLE SYSTEM (1)
  WHERE
    fct_registros.velocidad > 0
  "
)
```
```{r, Distribucion de velocidad Barras, fig.cap= "Distribucion de velocidad promedio"}
velocidad_media <- mean(velocidad_volumen$velocidad)
velocidad_desvio <- sd(velocidad_volumen$velocidad)

velocidad_volumen %>% ggplot() +
  geom_density(aes(x = velocidad), fill = "blue", alpha = 0.5, kernel = "epanechnikov") +
  geom_histogram(aes(x = velocidad, y = after_stat(density) ), fill = "red", alpha = 0.5, bins = 144)+
  scale_x_continuous(name = "Velocidad promedio (km/h)") +
  scale_y_continuous(name = "Densidad") +
  theme(
    legend.position = 'none'
  ) +
  geom_vline(xintercept = velocidad_media, color = "black", linetype = "dashed", size = 1) +
  annotate("text", x = velocidad_media + 5, y = 0.02, label = paste("Velocidad promedio:", round(velocidad_media, 2), "km/h"), size = 3) +
  geom_vline(xintercept = velocidad_media + velocidad_desvio, color = "black", linetype = "dashed", size = 1) +
  geom_vline(xintercept = velocidad_media - velocidad_desvio, color = "black", linetype = "dashed", size = 1)
```

## Principales variables: Velocidad
Se puede observar que la distribución de la velocidad es normal, con una media de 31.31 km/h y un desvio de 17.08 km/h.
El 68% de los datos se encuentran entre 14.23 km/h y 48.39 km/h.

## Principales variables

### Volumen
Resumen de la variable volumen
```{r Resumen de la variable volumen, echo = FALSE, results='asis'}
options(xtable.comment = FALSE)
    resumen_volumen <- load_data("resumen_volumen.csv",
    con,
      "
        SELECT
          MIN(volume) AS minimo,
          PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY volume) AS cuartil_1,
          PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY volume) AS mediana,
          PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY volume) AS cuartil_3,
          MAX(volume) AS maximo,
          AVG(volume) AS promedio,
          STDDEV(volume) AS desvio
        FROM fct_registros
      "
    )
print(xtable::xtable(resumen_volumen))
```
Se ve que la cantidad mínima de vehículos registrados en 5 minutos es 0 y la máxima es 659.
El promedio de volumen es 17.28 vehículos y el desvio estándar es 19.18 vehículos.

Ahora veamos la distribución del volumen registrado
Para mejor visualización, vamos a dejar de lado los datos donde el volumen es 0. 
Se podra observar que la mayoria de los valores son menores a 100, en particular
el 75% de los datos son menores a `r round(resumen_volumen$cuartil_3, 2)` vehículos.


## Principales variables: Volumen
```{r, Distribucion de volumen Barras, fig.cap= "Distribucion de volumen"}
volumen_media <- mean(velocidad_volumen$volumen)
volumen_desvio <- sd(velocidad_volumen$volumen)

velocidad_volumen %>% ggplot() +
  geom_density(aes(x = volumen), fill = "blue", alpha = 0.5, kernel = "epanechnikov") +
  geom_histogram(aes(x = volumen, y = after_stat(density) ), fill = "red", alpha = 0.5, bins = 144)+
  scale_x_continuous(name = "Volumen de vehiculos") +
  scale_y_continuous(name = "Densidad") +
  theme(
    legend.position = 'none'
  ) +
  geom_vline(xintercept = volumen_media, color = "black", linetype = "dashed", size = 1) +
  annotate("text", x = volumen_media + 5, y = 0.02, label = paste("Volumen promedio:", round(volumen_media, 2), "vehiculos"), size = 3) +
  geom_vline(xintercept = volumen_media + volumen_desvio, color = "black", linetype = "dashed", size = 1) +
  geom_vline(xintercept = volumen_media - volumen_desvio, color = "black", linetype = "dashed", size = 1) +
  coord_cartesian(xlim = c(0, 150))
```



## Preguntas de Investigacion

Las preguntas que dirigirán este análisis son las siguientes:

1. ¿Existe alguna correlación entre el volumen y la velocidad?.

2.  ¿Cuáles son las calles con los mayores promedios de velocidad en Montevideo? ¿Con que frecuencia se cometen excesos de velocidad?.

3. ¿Cómo va variando el volumen y velocidad medidos a través del TIEMPO?.


## ¿Existe alguna correlación entre el volumen y la velocidad?

Haremos un grafico de puntos para visualizarlo.
Para los datos usaremos una muestra aleatoria de toda la base de datos 
```{r Muestra y Plot, fig.cap="Grafico de puntos de velocidad y volumen"}

velocidad_volumen_muestra <- velocidad_volumen %>% sample_n(nrow(velocidad_volumen)*0.3)

velocidad_volumen_muestra %>% ggplot() + geom_point(
  aes(
    x = velocidad,
    y= volumen
  )
)

```


## ¿Existe alguna correlación entre el volumen y la velocidad?
Definitivamente **no hay una relacion lineal** entre velocidad y volumen
```{r Correlacion de velocidad y volumen, results='asis'}
options(xtable.comment = FALSE)

print(xtable::xtable(cor(velocidad_volumen)))
```
La correlación dio `r round(cor(velocidad_volumen$velocidad, velocidad_volumen$volumen), 2)`, lo que indica que no hay una correlación lineal entre las variables.
 
 
## ¿Cuáles son las calles con los mayores promedios de velocidad en Montevideo?
Pasemos a investigar las calles con mayor promedio de velocidad.
```{r Importar datos de velocidad-calle, echo = FALSE }
velocidad_calles <- load_data( "velocidad_calles.csv",
  con,
  "
  SELECT d_sensores.dsc_avenida,
    AVG(velocidad) AS velocidad_promedio
  FROM fct_registros
  INNER JOIN d_sensores ON fct_registros.id_detector = d_sensores.id_detector
  GROUP BY dsc_avenida
  "
  )
```
```{r Calles con mayor promedio de velocidad, echo = FALSE, fig.cap="Calles con mayor promedio de velocidad"}
avenidas <- c("18 de Julio","8 de Octubre","Agraciada", "Av Brasil", "Av Italia","Bv Artigas","Bv Batlle y Ordonez","Bv Espana","C M Ramirez","Centenario","Fernandez Crespo","Bolivia","Albo","Belloni","Benito Blanco","Burges","Garibaldi","Garzon","Gral Flores","Millan","Morquio","Ponce","Rambla","Ricaldoni","Rivera","Rodo","San Martin","Saravia", "Soca","Uruguay","Varela", "L A de Herrera")  

velocidad_calles %>% 
  mutate(
    Tipo = ifelse(dsc_avenida %in% avenidas,
                  "Avenida", "Calle")
    ) %>% 
  arrange(desc(velocidad_promedio)) %>% 
  slice_head(n = 10) %>% 
  ggplot() +
  geom_col(
    aes(x = velocidad_promedio,
        y = reorder(dsc_avenida, velocidad_promedio),
        fill = Tipo) 
    ) +
  geom_text(
    aes( 
      x = velocidad_promedio, 
      y = dsc_avenida, 
      label = round(velocidad_promedio, 2)
    ),
    size = 5,
    nudge_x = 4,
  ) +
  labs(x = "Velocidad promedio",
       y = "Calle") +
  scale_fill_manual(
    values = c("Avenida" = "blue",
               "Calle" = "grey33")
    ) +
  theme_bw()
  
```

## ¿Cuáles son las calles con los mayores promedios de velocidad en Montevideo? ¿Con que frecuencia se cometen excesos de velocidad? 
El gráfico nos muestra que las 3 calles con mas velocidad en promedio superan el máximo de 45km/h siendo este la velocidad máxima de circulación reglamentaria. La avenida más rápida en promedio no alcanza el máximo de velocidad perimitido

En promedio de velocidad circulacion los conductores son prudentes, aun asi vemos con que frecuencia se comenten excesos de velocidad.

## ¿Con que frecuencia se cometen excesos de velocidad? 
Si bien las proporciones son de valor bajo tengamos en cuenta que estamos tomando el 1% de mas de 85 millones de registros, es decir que hay mas de 850 mil infracciones de velocidad solo en Avenida Rivera, en Avenida Italia hay mucha mas diferencia superando el millón cuatrocientos en casi 2 años y medio.
```{r Importar datos de exceso de velocidad, echo=FALSE }
excesos <- load_data( "excesos.csv",
  con,
  "
  SELECT d_sensores.dsc_avenida,
    fct_registros.velocidad 
  FROM fct_registros TABLESAMPLE SYSTEM (10)
  JOIN d_sensores ON fct_registros.id_detector = d_sensores.id_detector
  "
  ) %>% 
  mutate(
    Tipo = ifelse(dsc_avenida %in% avenidas,
                  "Avenida", "Calle"),
    limite = ifelse(dsc_avenida %in% c("Bv Artigas",
                                       "Rambla",
                                       "Larranaga",
                                       "Bv Batlle y Ordonez",
                                       "Garzon"), 60, 45)
  )
```
```{r Grafico, fig.cap = "Propocion de excesos de velocidad por calle"}
excesos %>% 
  filter(velocidad > limite) %>% 
  group_by(dsc_avenida) %>%  
  summarise(
    cant = (n()/nrow(excesos))*100
    ) %>%
  arrange(desc(cant)) %>% 
  slice_head(n = 8) %>% 
  ggplot() +
  geom_col(
    aes(
      x = cant, 
      y = reorder(dsc_avenida, cant))
    ) + 
  geom_text(
    aes( 
      x = cant, 
      y = dsc_avenida, 
      label = paste(round(cant, 2),"%")
      ),
    size = 5,
    nudge_x = 0.15
  ) +
  labs(x = "Proporcion",
       y = "Calle") +
  theme_bw()
```


## ¿Cómo va variando el volumen y velocidad medidos a traves de el tiempo?
```{r registros_maximos_hora, echo = FALSE}
registros_maximos_hora <- load_data("registros_maximos_hora.csv",
    con,
    "
      SELECT
        d_date.day_of_week,
        d_date.day_name,
        d_date.month_actual,
        d_date.month_name,
        d_date.year_actual,
        d_date.weekend_indr,
        CASE
          WHEN fct_registros.id_hora >= 0 AND fct_registros.id_hora < 100 THEN '00:00'
          WHEN fct_registros.id_hora >= 100 AND fct_registros.id_hora < 200 THEN '01:00'
          WHEN fct_registros.id_hora >= 200 AND fct_registros.id_hora < 300 THEN '02:00'
          WHEN fct_registros.id_hora >= 300 AND fct_registros.id_hora < 400 THEN '03:00'
          WHEN fct_registros.id_hora >= 400 AND fct_registros.id_hora < 500 THEN '04:00'
          WHEN fct_registros.id_hora >= 500 AND fct_registros.id_hora < 600 THEN '05:00'
          WHEN fct_registros.id_hora >= 600 AND fct_registros.id_hora < 700 THEN '06:00'
          WHEN fct_registros.id_hora >= 700 AND fct_registros.id_hora < 800 THEN '07:00'
          WHEN fct_registros.id_hora >= 800 AND fct_registros.id_hora < 900 THEN '08:00'
          WHEN fct_registros.id_hora >= 900 AND fct_registros.id_hora < 1000 THEN '09:00'
          WHEN fct_registros.id_hora >= 1000 AND fct_registros.id_hora < 1100 THEN '10:00'
          WHEN fct_registros.id_hora >= 1100 AND fct_registros.id_hora < 1200 THEN '11:00'
          WHEN fct_registros.id_hora >= 1200 AND fct_registros.id_hora < 1300 THEN '12:00'
          WHEN fct_registros.id_hora >= 1300 AND fct_registros.id_hora < 1400 THEN '13:00'
          WHEN fct_registros.id_hora >= 1400 AND fct_registros.id_hora < 1500 THEN '14:00'
          WHEN fct_registros.id_hora >= 1500 AND fct_registros.id_hora < 1600 THEN '15:00'
          WHEN fct_registros.id_hora >= 1600 AND fct_registros.id_hora < 1700 THEN '16:00'
          WHEN fct_registros.id_hora >= 1700 AND fct_registros.id_hora < 1800 THEN '17:00'
          WHEN fct_registros.id_hora >= 1800 AND fct_registros.id_hora < 1900 THEN '18:00'
          WHEN fct_registros.id_hora >= 1900 AND fct_registros.id_hora < 2000 THEN '19:00'
          WHEN fct_registros.id_hora >= 2000 AND fct_registros.id_hora < 2100 THEN '20:00'
          WHEN fct_registros.id_hora >= 2100 AND fct_registros.id_hora < 2200 THEN '21:00'
          WHEN fct_registros.id_hora >= 2200 AND fct_registros.id_hora < 2300 THEN '22:00'
          WHEN fct_registros.id_hora >= 2300 AND fct_registros.id_hora <= 2359 THEN '23:00'
        ELSE 'Unknown'
      END AS hora_rango,
        MAX(fct_registros.velocidad) AS max_velocidad,
        MAX(fct_registros.volume) AS max_volumen,
        AVG(fct_registros.velocidad) AS promedio_velocidad,
        AVG(fct_registros.volume) AS promedio_volumen,
        MAX(fct_registros.volumen_hora) as max_volumen_hora,
        AVG(fct_registros.volumen_hora) as promedio_volumen_hora,
        COUNT(fct_registros.velocidad) AS cant_registros
    FROM fct_registros
    INNER JOIN d_sensores ON fct_registros.id_detector = d_sensores.id_detector
    LEFT JOIN d_date ON fct_registros.id_fecha = d_date.id_fecha
    GROUP BY d_date.day_of_week, hora_rango, d_date.month_actual, d_date.year_actual, d_date.weekend_indr, d_date.day_name, d_date.month_name
    "
  )
```
```{r Datos para mapa de calor}
max_velocidad_hora_barrio <- load_data("max_velocidad_hora_barrio.csv",
    con,
    "
    SELECT
      d_sensores.barrio,
      MAX(fct_registros.velocidad) AS max_velocidad,
      AVG(fct_registros.velocidad) AS avg_velocidad,
      MAX(fct_registros.volume) AS max_volumen,
      AVG(fct_registros.volume) AS avg_volumen,
       CASE
        WHEN fct_registros.id_hora >= 0 AND fct_registros.id_hora < 100 THEN '00:00'
        WHEN fct_registros.id_hora >= 100 AND fct_registros.id_hora < 200 THEN '01:00'
        WHEN fct_registros.id_hora >= 200 AND fct_registros.id_hora < 300 THEN '02:00'
        WHEN fct_registros.id_hora >= 300 AND fct_registros.id_hora < 400 THEN '03:00'
        WHEN fct_registros.id_hora >= 400 AND fct_registros.id_hora < 500 THEN '04:00'
        WHEN fct_registros.id_hora >= 500 AND fct_registros.id_hora < 600 THEN '05:00'
        WHEN fct_registros.id_hora >= 600 AND fct_registros.id_hora < 700 THEN '06:00'
        WHEN fct_registros.id_hora >= 700 AND fct_registros.id_hora < 800 THEN '07:00'
        WHEN fct_registros.id_hora >= 800 AND fct_registros.id_hora < 900 THEN '08:00'
        WHEN fct_registros.id_hora >= 900 AND fct_registros.id_hora < 1000 THEN '09:00'
        WHEN fct_registros.id_hora >= 1000 AND fct_registros.id_hora < 1100 THEN '10:00'
        WHEN fct_registros.id_hora >= 1100 AND fct_registros.id_hora < 1200 THEN '11:00'
        WHEN fct_registros.id_hora >= 1200 AND fct_registros.id_hora < 1300 THEN '12:00'
        WHEN fct_registros.id_hora >= 1300 AND fct_registros.id_hora < 1400 THEN '13:00'
        WHEN fct_registros.id_hora >= 1400 AND fct_registros.id_hora < 1500 THEN '14:00'
        WHEN fct_registros.id_hora >= 1500 AND fct_registros.id_hora < 1600 THEN '15:00'
        WHEN fct_registros.id_hora >= 1600 AND fct_registros.id_hora < 1700 THEN '16:00'
        WHEN fct_registros.id_hora >= 1700 AND fct_registros.id_hora < 1800 THEN '17:00'
        WHEN fct_registros.id_hora >= 1800 AND fct_registros.id_hora < 1900 THEN '18:00'
        WHEN fct_registros.id_hora >= 1900 AND fct_registros.id_hora < 2000 THEN '19:00'
        WHEN fct_registros.id_hora >= 2000 AND fct_registros.id_hora < 2100 THEN '20:00'
        WHEN fct_registros.id_hora >= 2100 AND fct_registros.id_hora < 2200 THEN '21:00'
        WHEN fct_registros.id_hora >= 2200 AND fct_registros.id_hora < 2300 THEN '22:00'
        WHEN fct_registros.id_hora >= 2300 AND fct_registros.id_hora <= 2359 THEN '23:00'
        ELSE 'Unknown'
      END AS hora_rango
    FROM fct_registros
    INNER JOIN d_sensores ON
      fct_registros.id_detector = d_sensores.id_detector
    LEFT JOIN d_date ON
      fct_registros.id_fecha = d_date.id_fecha
    GROUP BY d_sensores.barrio, hora_rango
    "
  ) 
```

```{r}
tabla <- load_data( "tabla.csv",
  con,
  "
  SELECT
    fct_registros.volume, 
    fct_registros.velocidad,
    d_date.day_of_week
  FROM
    fct_registros TABLESAMPLE SYSTEM (0.5)
  INNER JOIN 
    d_sensores ON fct_registros.id_detector = d_sensores.id_detector
  INNER JOIN 
    d_date ON fct_registros.id_fecha = d_date.id_fecha
  WHERE
    fct_registros.velocidad <> 0 AND fct_registros.volume <> 0

  "
) %>% 
  dplyr::mutate( 
    dia_de_la_semana = 
      factor(
        day_of_week,
        levels = c(1, 2, 3, 4, 5, 6, 7),
        labels = c("Lunes", "Martes", "Miercoles", "Jueves", "Viernes", "Sabado", "Domingo")
      )
  )
```

```{r promedios_semanales, echo=FALSE}
promedios_semanales <- load_data("promedios_semanales.csv",
  con,
  "
  WITH tabla as (
    SELECT
      d_date.day_of_week as dia,
      fct_registros.velocidad as velocidad,
      fct_registros.volume as volumen
    FROM fct_registros
    LEFT JOIN d_date ON fct_registros.id_fecha = d_date.id_fecha
    )
    
    
    SELECT
      avg(velocidad) as avg_velocidad,
      avg(volumen) as avg_volumen,
      dia
    FROM tabla 
    GROUP BY dia
    "
  ) %>%
  mutate( 
    dia_semana = 
      factor(
        dia,
        levels = c(1, 2, 3, 4, 5, 6, 7),
        labels = c("Lunes", "Martes", "Miercoles",
                   "Jueves", "Viernes", "Sabado", "Domingo")
        ) 
    )
```
Primero observemos como va variado volumen 

## VOlumen

#### Variacion por hora del dia
Vamos a ver que en Parque Rodo hay valores altos durante la noche, que lo diferencia del resto de los barrios.
```{r Mapa de calor, fig.cap="Mapa de calor de volumen maxima por barrio por rango de hora"}
max_velocidad_hora_barrio %>%
  ggplot(
    aes(
      x = barrio,
      y = hora_rango,
      fill = avg_volumen
    )
  ) +
  geom_tile() +
  coord_fixed() +
  coord_flip() +
  scale_x_discrete( name = "Barrio") +
  scale_y_discrete( name = "Hora") +
  scale_fill_continuous(
    name =  "AVG Volumen",
    type = "viridis"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
    axis.text.y = element_text(size = 5)
  )


```





## Volumen

#### Variacion del volumen por dia de la semana
Se puede notar un volumen claramente más alto los días de semana, yendo en aumento de lunes a viernes, hasta bajar el promedio los fines de semana. 
El día de la semana con más volumen de tránsito es el viernes, y el de menor volumen es el domingo.
```{r}
promedios_semanales %>% ggplot() +
  geom_col(
    aes(
      x = dia_semana,
      y = avg_volumen
    ),
    fill = "black"
  ) + 
  geom_text(
    aes( 
      x = dia_semana, 
      y = avg_volumen, 
      label = round(avg_volumen, 2)
    ),
    color = "white",
    vjust = 5, 
    size = 8
  ) +
  labs(
    x = "Dia de la semana",
    y = "Volumen Promedio"
  ) + 
  theme(aspect.ratio = 0.4)
```

## Volumen

#### Variacion de la velocidad por dia de la semana
En el siguiente gráfico se puede notar un comportamiento casi idéntico en la cantidad de observaciones del volumen, según el día de la semana. La mayoría de los datos se concentra cuando el volumen está entre 0 y 10, la mayoría de los días. Los fines de semana (En especial el domingo) se puede observar un comportamiento distinto, ya que cuando el volumen está entre 0 y 10, la cantidad de observaciones es mucho mayor, y no hay tanta concentración de datos cuando el volumen aumenta.

## Volumen

#### Variacion de la velocidad por dia de la semana
```{r, echo = FALSE, fig.cap="Densidad de Volumen"}
tabla %>%
  ggplot() + 
  geom_density(
    aes(x = volume),
    kernel = "epanechnikov"
  ) +
  facet_wrap(~dia_de_la_semana) +
  scale_x_continuous(
    name = "Volumen",
    breaks = seq(0,100,10)
  ) +
  scale_y_continuous(
    name= "Cantidad"
  ) +
  coord_cartesian(xlim = c(0, 100))
```

## Veamos ahora la variable velocidad


Podemos observar que el promedio de velocidad es casi el mismo para cada día de la semana, a excepción de los fines de semana, donde la velocidad aumenta levemente. Esto concuerda con la disminución del volumen los fines de semana, visto anteriormente.
```{r}
promedios_semanales %>% ggplot() +
  geom_col(
    aes(
      x = dia_semana,
      y = avg_velocidad,
    )
  ) + 
  geom_text(
    aes( 
      x = dia_semana, 
      y = avg_velocidad, 
      label = round(avg_velocidad, 2) 
    ),
    vjust = -0.5
  ) +
  labs(
    x = "Dia de la semana",
    y = "Promedio"
  ) 
```

## Velocidad

### Velocidad por dia de la semana
Viendo la densidad de la velocidad parece no variar a lo largo de los días de la semana. Alcanzan su valor máximo alrededor de los 35km/h.
```{r, echo = FALSE, fig.cap="Densidad de Velocidad"}
tabla %>%
  ggplot() + 
  geom_density(
    aes(x = velocidad),
    kernel = "epanechnikov"
  ) +
  facet_wrap(~dia_de_la_semana) + 
  scale_x_continuous(breaks = seq(0, 150, 20),
                     name = "Velocidad"
  ) +
  scale_y_continuous(
    name= "Densidad"
  )
```


## Velocidad

### Velocidades maximas registradas
Dentro de los días de la semana hemos observado diferentes comportamientos de la velocidad durante los días sábado y domingo (fin de semana), en particular observemos las velocidades máximas registradas de lunes a viernes y durante el fin de semana.


## Velocidad
```{r Grafico de distribucion max_velocidad por hora, echo = FALSE, fig.cap='Distribucion maxima velocidad por hora'}
mediana <- registros_maximos_hora %>% 
  summarise(mediana = median(max_velocidad))
mediana <- as.numeric(mediana)

my_labeller <- function(x) {
  ifelse(x, "Fin de semana", "Entre semana")
}

registros_maximos_hora %>%
  ggplot(
    aes(
      x = max_velocidad,
      y = fct_rev(hora_rango)
    )
  ) +
  geom_boxplot(
    aes(
      fill=hora_rango
    )
  ) +
  stat_summary(fun = mean, geom = "point", size = 3, alpha = 0.6) +
  geom_vline(xintercept = mediana) +
  facet_grid(
    ~weekend_indr,
    labeller = as_labeller(my_labeller)
  )+
  theme(
    legend.position = "none"
  ) +
  scale_x_continuous(
    name = "Velocidad Maxima Registrada"
  ) +
  scale_y_discrete(
    name= "Hora del día"
  )
```

## Velocidad

Este gráfico nos revela cosas interesantes, ya que podemos notar que los fines de semana, los máximos de velocidad registrados son mayores que los días de semana. De manera contraria, los máximos de velocidad con valores bajos son más comunes los días de semana.
Esto también está muy relacionado a lo visto anteriormente sobre el volumen, ya que los fines de semana el volumen es menor, y eso permite facilidad a alcanzar picos de velocidad mayores. Como los días de semana el volumen de tráfico es mayor, es esperable que los máximos de velocidad no escalen demasiado.


## Velocidad
Luego, con respecto a los meses, se puede ver que los dos meses en los cuales la densidad de la velocidad máxima es más similar los días de semana y los fines de semana son enero y febrero, y esto puede ser causado por las vacaciones de verano.
```{r Densidades de max_velocidad x mes del año, fig.cap='Densidad de la velocidad por meses'}
registros_maximos_hora %>%
  ggplot(
    aes(
      x =  max_velocidad,
      fill = weekend_indr # esto añade el color según el factor esFinDeSemana
    )
  ) +
  geom_density(
    alpha = 0.5,
    kernel = "epanechnikov"
  ) +
  facet_wrap(~month_actual)+
  scale_x_continuous(
    name = "Velocidad Maxima Registrada"
  )+
  scale_y_continuous(
    name = "Densidad"
  ) +
  scale_fill_discrete(
    name = "Dias de la semana",
    labels = c("Entre semana", "Fin de semana")
  )
  
```

## Velocidad

#### Variacion por año
Si entramos en cada año vemos como crece la densidad de velocidad máxima registrada de lunes a viernes. 
```{r, echo = FALSE, fig.cap = '2023 no esta completo'}
registros_maximos_hora %>%
  ggplot(
    aes(
      x =  max_velocidad,
      fill = weekend_indr
    )
  ) +
  geom_density(
    alpha = 0.5,
    kernel = "epanechnikov"
  ) +
  facet_grid(~year_actual) +
  theme(legend.position = "bottom")+
  scale_x_continuous(
    name = "Velocidad Maxima Registrada"
  )+
  scale_y_continuous(
    name = "Densidad"
  ) +
  scale_fill_discrete(
    name = "Dias de la semana",
    labels = c("Entre semana", "Fin de semana")
  )
  
```

## Velocidad


En 2021 y 2022, los días de semana hubo una densidad de velócidad máxima muy similar, aunque los fines de semana hubo mayor densidad en valores altos de velocidad en 2021.
En 2023 podemos ver una densidad mayor los días de semana, con menos diferencia de los días de semana como los años anteriores. 
Cabe aclarar que los datos de 2023 solo abarcan hasta mayo. Esto puede dar una explicación de la diferencia menor entre fin de semana y día de semana, ya que anteriormente vimos que los primeros meses del año muestran un comportamiento similar.


## Velocidad

### Variacion conjunta de velocidad y volumen
```{r}
registros_maximos_hora %>% 
  ggplot(
  aes(
    x= max_velocidad,
    y = promedio_volumen
  )
)  + 
  geom_point(
    color = "purple"
  ) + 
  theme(legend.position = "none")
  facet_wrap(~hora_rango)
```

## Velocidad

Se observa una relación negativa, ya que cuanto mayor es el volumen, menor es el máximo de velocidad, y cuando el volumen es menor, los picos de velocidad tienden a ser mucho mayores.

Además, si separamos por hora del día podemos ver que los momentos de mayor volumen de tráfico suelen darse a la tarde, entre las 16:00 y las 18:00. También, los momentos donde el volumen es menor, y los picos de velocidad mayores suelen darse en la madrugada, entre las 2:00 y las 4:00 como muestra el siguiente gráfico.

## Velocidad

### Variacion conjunta de velocidad y volumen
```{r}
registros_maximos_hora %>% 
  ggplot(
  aes(
    x= max_velocidad,
    y = promedio_volumen
  )
)  + 
  geom_point(
    aes(
      color = hora_rango
    )
  ) + 
  theme(legend.position = "none") + 
  facet_wrap(~hora_rango)
```


## Resultados interesantes

Hemos visto en general un promedio de velocidad constante a lo largo de los con mayor volumen de trafico durante los fines de semana casi debajo de los limites establecidos. No obstante existen registros de alta velocidad en masa con mayor frecuencia en la madrugada siendo registros que se toman en las zonas urbanas de Montevideo lo cual es llamativo

Hemos visto una ciudad con un volumen promedio de 17, si lo vemos barrio por barrio notaremos un volumen promedio entre 30 y 40 autos entre las 7 y las 18hs y muy poca circulación en la madrugada excepto el barrio Parque Rodo teniendo incluso mayor volumen de 0 a 4hs que el resto de las horas.


## Modelo estadístico
Para el diseño del modelo, nos pareció interesante evaluar la interacción entre el volumen y la velocidad, además de otros factores planteados en las preguntas iniciales, como la hora o el día de la semana. 
Para esto, observamos que estas variables están claramente correlacionadas, por lo cual no es viable hacer un modelo de regresión ya que es necesaria la independencia de los errores. Por esto, concluimos en que es una mejor opción hacer un arbol de decisión, ya que nos permite observar esta dependencia con más claridad.


## Predicir velocidad promedio de un sensor

Ya definido el tipo de modelo, nos resta definir la variable de respuesta y sus predictoras.
Nos pareció que la mejor opción para ser variable de respuesta era la velocidad, ya que vemos que cada uno de los otros factores son condicionantes para esta variable. 

Luego, nuestras variables predictoras serán el volumen, la hora y el día de la semana.

## Predicir velocidad promedio de un sensor

Para la hora y el día, decidimos seccionar las variables de forma binaria, ya que observamos en la velocidad una tendencia de comportamiento distinta en dos bloques bien definidos de cada variable. 
La hora estará seccionada en "día" y "noche" siendo "día" entre las 8:00 y las 20:00, y "noche" el caso contrario. 
Para el día de la semana, seccionaremos los datos en "fin de semana" y "día de semana", ya que el tráfico suele comportarse de maneras diferentes en cada caso.

Para el volumen y la velocidad, tomamos el promedio para simplificar los datos.
Los datos están agrupados por detector, hora, y día de la semana (Fin de semana o no).


## Predicir velocidad promedio de un sensor

### Nuestros datos:
```{r Datos para arbol, echo=FALSE, results='asis'}
options(xtable.comment = FALSE)

datos_arbol <- load_data("datos_arbol.csv",
  con,
  "
  SELECT
    AVG(fct_registros.volume) as avg_volumen,
    AVG(fct_registros.velocidad) as avg_velocidad,
    d_date.weekend_indr as esFinDeSemana,
    CASE
            WHEN fct_registros.id_hora >= 600 AND fct_registros.id_hora < 1900 THEN TRUE
            ELSE FALSE
    END AS esDeDia
FROM fct_registros
LEFT JOIN d_date ON fct_registros.id_fecha = d_date.id_fecha
GROUP BY
    fct_registros.id_detector,
    esFinDeSemana,
    esDeDia
  "
)
print(xtable::xtable(head(datos_arbol)))
```

## Predicir velocidad promedio de un sensor

Ya tenemos los datos, nos queda armar el arbol.
Primero, tomaremos una muestra para crear los conjuntos de entrenamiento y prueba. La proporción será de un 70% para entrenamiento y un 30% para prueba. Antes de tomar la muestra fijaremos una semilla para poder analizar el mismo modelo de forma reproducible.

## Predicir velocidad promedio de un sensor
```{r Arbol training}
set.seed(946)
intrain <- sample(x = 1:nrow(datos_arbol), size = nrow(datos_arbol)*0.7)

training <- datos_arbol[intrain,]

testing <- datos_arbol[-intrain,]

arbol_vol <- rpart(avg_velocidad ~ esfindesemana + esdedia + avg_volumen, data = training)
```
Al observar el modelo, notaremos que el volumen aparece repetidas veces seccionando los datos. Esto es porque, al ser continua, hay una variabilidad mucho más alta, y hay más casos para observar.
```{r Arbol Plot}
rpart.plot(arbol_vol, digits = (-4))

```

## Predicir velocidad promedio de un sensor
En concordancia con el comentario anterior, la variable volumen divide los datos en 4 categorías (Con límites 2.01; 3.93; 10.11) y la variable "esdedia" solo actúa una vez, mientras la del fin de semana ni siquiera es utilizada por el arbol.

Podemos observar que los datos donde el volumen es menor a 3.93 es una minoría, ya que abarcan poco más de una décima parte. De todas formas, hay una división marcada en estos datos, ya que si el volumen es menor a 2, la velocidad suele ser de 6km/h, mientras que cuando el volumen es mayor a 2, la velocidad aumenta a casi 21km/h (Además es más significativa la cantidad de datos).


## Predicir velocidad promedio de un sensor
Cuando el volumen es mayor a 3.93 también hay un límite que demarca un comportamiento distinto entre los datos que superan y no este número, y es el 10.11. 
Los datos que tienen un volumen menor a 10.11, abarcan casi un tercio de los datos con una velocidad promedio de 28.29km/h, y los datos con volumen mayor a 10.11 abarcan casi un 58% de los datos con una velocidad promedio de 33 km/h. Se observa que la velocidad es mayor cuando el volumen es mayor a 10.11, aunque no es una diferencia tan significativa.

Una observación interesante es la diferencia de comportamiento entre los datos del día y de noche según el volumen. En todos los casos la velocidad es considerablemente mayor de noche que de día, pero la cantidad de datos observados es mayor de noche si el volumen es menor a 10, pero es mayor de día si el volumen es mayor a 10. Es decir, cuando el volumen es menor, hay más observaciones de noche, pero cuando hay mucho volumen de tráfico, hay menos observaciones de noche y de día hay muchas más. 


## Predicir velocidad promedio de un sensor

Para saber la fiabilidad del modelo, calculamos el error cuadrado medio, y en base a eso fuimos ajustando los parámetros del modelo hasta llegar al actual, ya que es el que menor error tiene.
```{r Arbol testing}
rmse(arbol_vol, training)
rmse(arbol_vol, testing)
```
El modelo se aleja alrededor de esa cantidad de kilometros de los datos reales. Hay un mayor error en el conjunto de prueba, ya que son menos valores.




## ShinyApp

En la aplicacion hay tres pestañas:

```{=tex}
\begin{itemize}

  \item{Mapa}
  
  \item{Graficos}

\end{itemize}
```
## Mapa

En esta seccion se presenta un mapa de Montevideo, obtenido del paquete \texttt{geouy}. Para su realizacion se utilizo el paquete \texttt{leaflet}. En este mapa se puede visualizar ciertos resumenes de los datos de velocidad y cantidad de vehiculos.

```{=tex}
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{img/mapa.png}
\caption{Mapa}
\end{figure}
```
El mapa tiene dos capas posibles de visualizar, una es por barrios y la otra por sensores.
En cada una de estas capas puedes filtrar los datos por dia de la semana y por hora. Tambien puedes ir cambiando la variable de resumen que se muestra en el mapa.

## Grafico

En esta seccion se presenta un grafico de barras que muestra la variable de resumen que se seleccione por hora. Tambien se filtra por dia de la semana y por barrio.

```{=tex}
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{img/grafico.png}
\caption{Grafico}
\end{figure}
```

## Comentarios finales

En promedio Montevideo es una ciudad con velocidad promedio de casi 31 km/h, no podemos dejar pasar por alto la cantidad de excesos de velocidad a valores muy elevados que durante el fin de semana se producen mas.
Con ese valor de velocidad promedio y mirando tambien el volumen promedio de 17, podemos llegar a decir que Montevideo es una ciudad bastante lenta teniendo en consideración que las mediciones fueron hechas incluso en avenidas donde mas se circula, mucho mas en las famosas "hora pico" entre las 16 y 18hs
