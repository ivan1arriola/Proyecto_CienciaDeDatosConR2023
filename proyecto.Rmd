---
title: "Proyecto final"
author: "Iván Arriola, Federico Miquelerena, Damián Rovetta"
date: "12-07-2023"
output: pdf_document
bibliography: [packages.bib, bibliografia.bib]
nocite: "@*"
header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{H}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.pos = "H", out.extra = "")
```

```{r Librerias , include = FALSE}
library(tidyverse)
library(DBI)
library(RPostgres)
library(sf)
library(paletteer)
library(geouy)
library(spdep)
library(xtable)
library(rpart)
library(rpart.plot)
source(here::here("app", "utils.R"))
library(randomForest)
library(modelr)
```

```{r Variables de Entorno, eval=FALSE, echo=FALSE}
usethis::edit_r_environ(
  scope = "project"
)
```

# Introducción

Esto es un análisis descriptivo de los datos del tráfico de Montevideo, Uruguay.
Hemos tomado los registros desde enero de 2021 hasta mayo de 2023 y nuestro interés es saber el comportamiento de la velocidad y el volumen de tráfico (variables explicativas) dependiendo de varias variables que iremos desarrollando a lo largo de la investigación.

# Datos

### Descripción general de los datos

Todos los datos fueron sacados de Catalogo de Datos Abiertos de **gub.uy**.
En particular, los datos elegidos son los siguientes:

- [Conteo vehicular en las principales avenidas de Montevideo](https://catalogodatos.gub.uy/dataset/intendencia-montevideo-conteo-de-vehiculos-del-centro-de-gestion-de-la-movilidad)
- [Velocidad promedio vehicular en las principales avenidas de Montevideo](https://catalogodatos.gub.uy/dataset/intendencia-montevideo-velocidad-promedio-vehicular-en-las-principales-avenidas-de-montevideo)
- [Ubicación de sensores de medición de conteo vehículos](https://catalogodatos.gub.uy/dataset/intendencia-montevideo-ubicacion-de-sensores-de-medicion-de-conteo-vehiculos)

Los tres dataset son mantenidos por la Intendencia de Montevideo.

### Descripcion de variables

Originalmente los datos vienen presentados de la siguiente forma:

#### Conteo vehicular en las principales avenidas de Montevideo

- `cod_detector`: Numérico - ID de la cámara que monitorea un carril específico para detectar vehículos.
- `id_carril`: Numérico - Número del carril monitoreado (1, 2, 3, ...).
- `fecha`: Fecha, AAAA-MM-DD - Día en que se realizó la medición.
- `hora`: hh:mm:ss - Hora en que se realizó la medición.
- `dsc_avenida`: Texto - Nombre de la avenida donde se mide el tráfico.
- `dsc_int_anterior`: Texto - Nombre de la vía desde donde vienen los vehículos.
- `dsc_int_siguiente`: Texto - Nombre de la vía hacia donde se dirigen los vehículos.
- `latitud`: Float - Latitud del lugar de medición.
- `longitud`: Float - Longitud del lugar de medición.
- `volumen`: Numérico - Cantidad de vehículos detectados en el carril en los últimos 5 minutos.
- `volumen_hora`: Numérico - Cantidad de vehículos detectados en el carril en la última hora.

#### Velocidad promedio vehicular en las principales avenidas de Montevideo

- `cod_detector`: Numérico - ID de la cámara que monitorea un carril específico para detectar vehículos.
- `id_carril`: Numérico - Número del carril monitoreado (1, 2, 3, ...).
- `fecha`: AAAA-MM-DD - Día en que se realizó la medición.
- `hora`: hh:mm:ss - Hora en que se realizó la medición.
- `dsc_avenida`: Texto - Nombre de la avenida donde se mide el tráfico.
- `dsc_int_anterior`: Texto - Nombre de la vía desde donde vienen los vehículos.
- `dsc_int_siguiente`: Texto - Nombre de la vía hacia donde se dirigen los vehículos.
- `latitud`: Float - Latitud del lugar de medición.
- `longitud`: Float - Longitud del lugar de medición.
- `velocidad_promedio`: Numérico - Promedio de las velocidades de los vehiculos que circularon por el carril durante los últimos 5 minutos.

#### Ubicación de sensores de medición de conteo vehículos

- `dsc_avenida`: Texto - Nombre de la avenida donde se encuentra el sensor o cámara y donde se mide el tránsito.
- `dsc_int_anterior`: Texto - Nombre de la vía que forma el cruce desde donde vienen los vehículos.
- `dsc_int_siguiente`: Texto - Nombre de la vía que forma el cruce donde está el sensor. En general, el sensor se encuentra un poco antes de esta vía. El sentido de circulación será desde el cruce con `dsc_int_anterior` hacia el cruce con `dsc_int_siguiente`.
- `latitud`: Float - Coordenada que indica la latitud de la ubicación del sensor.
- `longitud`: Float - Coordenada que indica la longitud de la ubicación del sensor.

Sobre estos datos en particular, son _100 sensores_ que se van cambiando de ubicación mes a mes.

## Base de datos

Debido a que los datos están estrechamente relacionados y a su vez son sumamente masivos, hemos decidido utilizar una base de datos quedando de la siguiente manera.

```{r Coneccion a Base de Datos, include = FALSE}
con <- DBI::dbConnect(
  RPostgres::Postgres(),
  host = Sys.getenv("DB_HOST"),
  port = Sys.getenv("DB_PORT"),
  user = Sys.getenv("DB_USER"),
  password = Sys.getenv("DB_PASS"),
  dbname = Sys.getenv("DB_NAME")
)
```

![Diagrama de la base de datos](app/media/fct_registros.png "Diagrama de la base de datos"){width="300" height="600"}

Nuestra tabla principal será `fct_registros`.

### Tabla: fct_registros

- Cantidad de datos: 85386695.
- Variables de la tabla:
  - `id_registros`: _Numérico_ (_Primary Key_).
  - `id_carril`: _Numérico_.
  - `id_fecha`: _Numérico_ (_Foreign Key_, vinculado con `d_sensores`). La fecha de la que fue tomada el registro, tiene el formato _YYYY-MM-DD_
  - `id_hora`: _Numérico_. Hora en la que fue tomado el registro con formato _HHMM_.
  - `id_detector`: _Numérico_ (_Foreign Key_, cinculado con `d_date`).
  - `volume`: _Numérico_. Cantidad de vehiculos que pasaron en los últimos 5 minutos.
  - `volumen_hora`: _Numérico_. Cantidad de vehiculos que pasaron en la ultima hora.
  - `velocidad`: _Numérico_. Velocidad promedio de los vehiculos registrados en los utimos 5 minutos. Unidad en km/h

### Tabla: d_sensores

- Cantidad de datos: 273
- Variables de la tabla:
  - `id_detector`: _Numérico_ (_Primary Key_).
  - `dsc_avenida`: _Texto_. Calle donde se encuentra el sensor.
  - `dsc_int_anterior`: _Texto_. Cruce previo de la calle en `dsc_avenida`.
  - `dsc_int_siguiente`: _Texto_. Cruce posterior de la calle en `dsc_avenida`. Estas dos juntas nos dirá que cada sensor se encuentra en _Avenida_ entre _Anterior_ y _Siguiente_.
  - `latitud`: _Numérico continuo_.
  - `longitud`: _Numérico continuo_. Junto a `latitud` nos indica las coordenadas geograficas del sensor.
  - `barrio`: _Texto_. Esta variable fue creada a partir del paquete `geouy`

### Tabla: d_date

- Cantidad de datos: 3652
- Variables de la tabla:
  - `id_fecha`: _Numérico_ (_Primary Key_)
  - `date_actual`: _Fecha_. Secuencia de fechas desde el 01-01-2021 con formato _YYYY-MM-DD_
  - `epoch`
  - `day_suffix`: _Texto_. Fecha del dia abreviado.
  - `day_name`: _Texto_. Nombre del día
  - `day_of_week`: _Numérico_. Dia de la semana que indica 1 como lunes, 2 como martes, etc.
  - `day_of_month`: _Numérico_. Fecha del mes, va desde 1 hasta 31.
  - `day_of_quarter`: _Numérico_. Dia del cuatrimestre.
  - `day_of_year`: _Numérico_. Dia del año, del 1 al 366.
  - `week_of_month`: _Numérico_. Semana de cada mes, valores del 1 al 5.
  - `week_of_year`: _Numérico_. Semana del año, valores del 1 al 53.
  - `week_of_year_iso`: _Texto_. Variable que combina el año, la semana del año y el día de la semana.
  - `month_actual`: _Numérico_. Mes del año tomado como numero, enero como 1, febrero como 2 y así sucesivamente.
  - `month_name`: _Texto_. Mes del año traducido en texto, de enero a diciembre
  - `month_name_abbreviated`: _Texto_. Mes del año en formato abreviado.
  - `quarter_actual`: _Numérico_. Indica el cuatrimestre correspondiente con numeros del 1 al 4.
  - `quarter_name`: _Texto_. Indica el cuatrimestre en formato de texto, primero, segundo, tercero y cuarto.
  - `year_actual`: _Numérico_. Indica el año.
  - `first_day_of_week`: _Fecha_. Indica el primer día de la semana que corresponde tal fecha.
  - `last_day_of_week`: _Fecha_. Indica el ultimo día del rango de la semana correspondiente.
  - `first_day_of_month`: _Fecha_. Limite inferior que indica a que mes corresponde cada fecha.
  - `last_day_of_month`: _Fecha_. Limite superior que indica a que mes corresponde cada fecha.
  - `first_day_of_quarter`: _Fecha_. Limite inferior que indica a que cuatrimestre corresponde cada fecha.
  - `last_day_of_quarter`: _Fecha_. Limite superior que indica a que cuatrimestre corresponde cada fecha.
  - `first_day_of_year`: _Fecha_. Limite inferior que indica a que año corresponde cada fecha.
  - `last_day_of_year`: _Fecha_. Limite superior que indica a que año corresponde cada fecha.
  - `mmyyyy`: _Numérico_. Secuencia de caracteres que indica el mes y el año en formato MMYYY
  - `mmddyyyy`: _Numérico_. Secuencia de caracteres que indica el mes, la fecha y el año en formato MMDDYYY.
  - `weekend_indr`: _Lógico_. `TRUE` si la fecha tiene como dia de la semana sabado o domingo, `FALSE` en caso contrario.
  - `feriado`: _Lógico_. `TRUE` si la fecha correspondiente coincide con dias feriados en Uruguay, `FALSE` en caso contrario.

# Análisis exploratorio

En nuestro proyecto tenemos datos que tienen una dimension geo-espacial, por lo que es importante tener en cuenta
que la información que tenemos no es homogenea. Tambien es importante tener en cuenta que la información que tenemos
es de un periodo de tiempo acotado.

Dicho esto, para empezar, me parecio adecuado comprobar la integridad de los datos, es decir, ver si tenemos datos
faltantes o datos que no tienen sentido.

## Datos faltantes y nulos

```{r Cantidad de datos faltantes y nulos, echo = FALSE}
# Cantidad de datos faltantes y nulos
tabla_datos_null <- load_data('tabla_datos_null.csv',con,
"
SELECT 'velocidad' AS atributo,
       COUNT(*) AS cant_total,
       SUM(CASE WHEN velocidad IS NULL THEN 1 ELSE 0 END) AS cant_null,
       SUM(CASE WHEN velocidad = 0 THEN 1 ELSE 0 END) AS cant_0
FROM fct_registros
UNION ALL
SELECT 'volumem_hora' AS atributo,
       COUNT(*) AS cant_total,
       SUM(CASE WHEN volumen_hora IS NULL THEN 1 ELSE 0 END) AS cant_null,
       SUM(CASE WHEN volumen_hora = 0 THEN 1 ELSE 0 END) AS cant_0
FROM fct_registros
UNION ALL
SELECT 'volume' AS atributo,
       COUNT(*) AS cant_total,
       SUM(CASE WHEN volume IS NULL THEN 1 ELSE 0 END) AS cant_null,
       SUM(CASE WHEN volume = 0 THEN 1 ELSE 0 END) AS cant_0
FROM fct_registros
UNION ALL
SELECT 'id_fecha' AS atributo,
       COUNT(*) AS cant_total,
       SUM(CASE WHEN id_fecha IS NULL THEN 1 ELSE 0 END) AS cant_null,
       SUM(CASE WHEN id_fecha = 0 THEN 1 ELSE 0 END) AS cant_0
FROM fct_registros
UNION ALL
SELECT 'id_detector' AS atributo,
       COUNT(*) AS cant_total,
       SUM(CASE WHEN id_detector IS NULL THEN 1 ELSE 0 END) AS cant_null,
       0 AS cant_0
FROM fct_registros;
") %>%
  mutate(
    porc_null = round(cant_null / cant_total * 100, 6),
    porc_0 = round(cant_0 / cant_total * 100, 6)
  )
print(tabla_datos_null)
```

En `r tabla_datos_null[5,3]` datos se perdio la informacion de la ubicacion del sensor, por lo que no sabemos de que calle se trata.
En el `r tabla_datos_null[1,6]`% de los datos se detecto velocidad 0 y en el mismo porcentaje se detecto volumen 0.

```{r, echo = FALSE}
# Cantidad de datos que tienen los tres campos en 0
datos_0 <- load_data('datos_0.csv',con,
"
SELECT
    COUNT(CASE WHEN fct_registros.velocidad = 0 AND fct_registros.volume = 0 AND fct_registros.volumen_hora = 0 THEN 1 END) AS registros_cero
FROM
    fct_registros
") 
```
Se descubrio que la cantidad de datos que tienen los tres campos en 0 es `r datos_0[1,1]` registros, lo que representa
el `r round(datos_0[1,1] / tabla_datos_null[1,4] * 100, 6)`% de los datos nulos.

```{r, echo = FALSE}
# Cantidad de datos que tienen los tres campos en 0 y no tienen id_detector
datos_0_null <- load_data('datos_0_null.csv',con,
"
SELECT
    COUNT(CASE WHEN fct_registros.velocidad = 0 AND fct_registros.volume = 0 AND fct_registros.volumen_hora = 0 AND fct_registros.id_detector IS NULL THEN 1 END) AS registros_cero_null
FROM
    fct_registros
")

# Cantidad de datos que tienen los tres campos en 0 y tienen id_detector
datos_0_not_null <- load_data('datos_0_not_null.csv',con,
"
SELECT
    COUNT(CASE WHEN fct_registros.velocidad = 0 AND fct_registros.volume = 0 AND fct_registros.volumen_hora = 0 AND fct_registros.id_detector IS NOT NULL THEN 1 END) AS registros_cero_not_null
FROM
    fct_registros
")
```

De los `r datos_0[1,1]` registros que tienen los tres campos en 0, `r datos_0_null[1,1]` no tienen id_detector y `r datos_0_not_null[1,1]` si tienen id_detector.

No queda claro si los datos que tienen los tres campos en 0 son datos que representan que no paso ningun vehiculo por el sensor o si son datos que no se pudieron obtener.
  



Sobre los datos faltantes de la ubicacion del sensor, son datos que no se pueden recuperar, por lo que se tendran
que descartar.

Se quiso averiguar en que fecha se perdio la informacion de la ubicacion del sensor, para ver si se podia recuperar
la informacion de otra forma, pero no se pudo.

```{r, echo = FALSE}
# Fecha en la que se perdio la informacion de la ubicacion del sensor
load_data('fecha_null.csv',con,
"
SELECT id_fecha,
       COUNT(*) AS cant_total,
       SUM(CASE WHEN id_detector IS NULL THEN 1 ELSE 0 END) AS cant_null
FROM fct_registros
GROUP BY id_fecha
HAVING SUM(CASE WHEN id_detector IS NULL THEN 1 ELSE 0 END) > 0
ORDER BY id_fecha;
") %>%
  mutate(
    porc_null = round(cant_null / cant_total * 100, 6)
  )
```

```{r, echo = FALSE}
datos_null <- load_data('datos_null.csv',
  con,
  "SELECT *
  FROM fct_registros
  WHERE id_detector IS NULL
  "
)

datos_null$fecha <- as.Date(as.character(datos_null$id_fecha), format = "%Y%m%d")
datos_null$hora <- datos_null$id_hora

summary(datos_null)

```

Se puede ver que los datos faltantes de la ubicacion del sensor van desde el 24/07/2021 hasta el 31/07/2021.
Tambien se puede ver que todos los datos faltantes son del carril 2.
Quiza se podrian recuperar los datos de la ubicacion del sensor revisando los datos originales pero es irrelevante
ya que son pocos datos y no afectan al analisis.


### Datos faltantes en la base de datos

```{r Meses presentes en la base de datos}
(
  registros_año_mes <- load_data("registros_año_mes.csv",
    con,
    "
       SELECT
        count(*),
    d_date.month_actual, d_date.year_actual
    FROM fct_registros
    LEFT JOIN d_date ON fct_registros.id_fecha = d_date.id_fecha
    GROUP BY d_date.month_actual, d_date.year_actual

    "
  )
)
```
Haciendo una observación de los meses cubiertos por los datos, nos hemos dado cuenta que faltan algunos meses en la base de datos


## Ubicacion de los sensores

Los datos que tenemos son de 100 sensores ubicados todos en Montevideo y estos van cambiando de ubicacion cada mes.
Por lo que el primer paso es ver cuantas ubicaciones distintas tenemos y cuantos sensores hay en cada ubicacion.

```{r Cargar mapas y sensores,  message=FALSE, results='hide'}
d_sensores <- load_data("d_sensores.csv",
  con,
  "SELECT * FROM d_sensores"
  )
mvd_map <- load_geouy("Barrios")
mvd_map_fixed <- st_make_valid(st_transform(mvd_map, crs = 4326))
puntos_sensores <- d_sensores %>%
  select(barrio, latitud, longitud) %>%
  mutate(transformarCoord(latitud, longitud, mvd_map))
```

Para mostrarlo, hemos decidido utilizar un mapa de Montevideo con los barrios y mostramos la cantidad de sensores ubicados en el.
En el mapa se puede ver que los sensores estan ubicados en 42 de los 62 barrios de Montevideo.
Los barrios que tienen sensores son 42 sobre 62 siendo los barrios de Buceo, Centro, Pocitos y Unión con mas de 20 sensores.

```{r Cantidad de sensores por barrio, echo = FALSE, fig.cap = 'Mapa de Montevideo con cantidad de sensores por barrio.'}
cant_sensores <- d_sensores %>%
  group_by(barrio) %>%
  summarise(
    cant_de_sensores = n()
    ) %>%
  arrange(desc(cant_de_sensores))

mvd_map_sensores <- mvd_map_fixed %>%
  left_join(cant_sensores, by = c("nombbarr" = "barrio")) %>%
  mutate(cant_de_sensores = ifelse(is.na(cant_de_sensores), 0, cant_de_sensores))

ggplot() +
  geom_sf(
    data = mvd_map_sensores,
    aes(
      fill = ifelse(cant_de_sensores == 0, NA, cant_de_sensores)
    )
  ) +
  scale_fill_viridis_c(option = "plasma", name = "Cantidad de sensores por barrio") +
  theme_void() +
  theme(legend.position = "bottom")
```

Ahora quiero mostrar la cantidad de datos que tenemos por ubicacion, para ver si hay alguna ubicacion en particular que
tenga mas o menos datos que las demas.

```{r Cantidad de datos por ubicacion, echo = FALSE}
cant_datos_por_ubicacion <- load_data("cant_datos_por_ubicacion.csv",
  con,
  "
    SELECT
      id_detector,
      d_date.mmyyyy,
      COUNT(*) AS cant_datos
    FROM fct_registros
    LEFT JOIN d_date on d_date.id_fecha = fct_registros.id_fecha
    GROUP BY id_detector, d_date.mmyyyy
  "
) %>% inner_join(
  d_sensores %>% select(barrio, id_detector), by = c("id_detector" = "id_detector")
) %>% inner_join(
  cant_sensores, by = c("barrio" = "barrio")
)
```

```{r PLOT Cantidad de datos por ubicacion, echo = FALSE, fig.cap = 'Cantidad de datos por ubicacion'}
cant_datos_por_barrio <- cant_datos_por_ubicacion %>%
  group_by(barrio) %>%
  summarise(
    cant_datos = sum(cant_datos)
  ) %>%
  arrange(desc(cant_datos))

cant_datos_por_barrio %>%
  ggplot(aes(x = fct_reorder(barrio, cant_datos), y = cant_datos, fill = barrio)) +
  geom_bar(stat = "identity", position = "stack") +
  theme(axis.text.x = element_text(size= 5, angle = 45, hjust = 1)) +
  scale_x_discrete(name = "Ubicacion") +
  scale_y_continuous(labels = scales::comma, name = "Cantidad de datos") +
  theme(
    legend.position = 'none'
  )

```

Se puede observar que la cantidad de datos por ubicacion no es para nada homogenea.
Los barrios con mayor cantidad de datos aportados al dataset son Union, Pocitos, Tres Cruces, Buceo y Centro.
Por otro lado Maroñas, Carrasco, Las Canteras y Cerrito son los que menos datos aportan.

Ahora me interesaria saber cuales son los barrios mejores representados en el dataset, es decir, cuales son los barrios
que tienen mas datos por metro cuadrado.

primero calculo el area de cada barrio y luego calculo la cantidad de datos por metro cuadrado.

```{r Mapa Montevideo con Area, echo = FALSE}
mvd_map_con_area <- mvd_map_fixed %>%
  mutate(
    area = as.numeric(st_area(mvd_map_fixed))
  ) %>% arrange(area)

```

```{r Cantidad de datos por ubicacion ponderado por area, echo = FALSE}
cant_datos_por_barrio_area <- cant_datos_por_ubicacion %>%
  group_by(barrio) %>%
  summarise(
    cant_datos = sum(cant_datos)
  ) %>%
  inner_join(
    mvd_map_con_area %>% select(nombbarr, area), by = c("barrio" = "nombbarr")
  ) %>%
  mutate(
    cant_datos_por_area = cant_datos / area
  ) %>%
  arrange(desc(cant_datos_por_area))
```

```{r PLOT Cantidad de datos por ubicacion ponderado por area, echo = FALSE, fig.cap = 'Cantidad de datos por ubicacion ponderado por area'}
cant_datos_por_barrio_area %>%
  ggplot(aes(x = fct_reorder(barrio, cant_datos_por_area), y = cant_datos_por_area, fill = barrio)) +
  geom_bar(stat = "identity", position = "stack") +
  theme(axis.text.x = element_text(size= 5, angle = 45, hjust = 1)) +
  scale_x_discrete(name = "Ubicacion") +
  scale_y_continuous(labels = scales::comma, name = "Cantidad de datos por metro cuadrado") +
  theme(
    legend.position = 'none'
  )

```

Se puede observar que los barrios con mayor cantidad de datos por metro cuadrado son Pocitos,
La Blanqueada, Jancito Vera, Centro y Tres Cruces.
Por otro lado Maroñas, Carrasco, Las Canteras, Peñarol y Cerrito son los que menos datos aportan por metro cuadrado.

## Principales variables

Las variables que se van a analizar son las siguientes: - `volume`: _Numérico_. Cantidad de vehiculos que pasaron en los últimos 5 minutos. - `volumen_hora`: _Numérico_. Cantidad de vehiculos que pasaron en la ultima hora. - `velocidad`: _Numérico_. Velocidad promedio de los vehiculos registrados en los utimos 5 minutos. Unidad en km/h

### Velocidad

Resumen de la variable velocidad

```{r Resumen de la variable velocidad, echo = FALSE}
(
    resumen_velocidad <- load_data("resumen_velocidad.csv",
    con,
      "
        SELECT
          MIN(velocidad) AS minimo,
          PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY velocidad) AS primer_cuartil,
          PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY velocidad) AS mediana,
          PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY velocidad) AS tercer_cuartil,
          MAX(velocidad) AS maximo,
          AVG(velocidad) AS promedio,
          STDDEV(velocidad) AS desvio
        FROM fct_registros
      "
    )
)
```







Ahora veamos la distribucion de la velocidad registrada
Para mejor visualizacion, voy a dejar de lado los datos donde la velocidad es 0

```{r Datos de velocidad y Volumen, echo = FALSE}
  velocidad_volumen <- load_data('velocidad_volumen.csv',
  con,
  "
  SELECT
    fct_registros.velocidad,
    fct_registros.volume AS volumen
  FROM
    fct_registros TABLESAMPLE SYSTEM (1)
  WHERE
    fct_registros.velocidad > 0
  "
)
```

```{r, Distribucion de velocidad Barras, fig.cap= "Distribucion de velocidad promedio"}
velocidad_media <- mean(velocidad_volumen$velocidad)
velocidad_desvio <- sd(velocidad_volumen$velocidad)

velocidad_volumen %>% ggplot() +
  geom_density(aes(x = velocidad), fill = "blue", alpha = 0.5, kernel = "epanechnikov") +
  geom_histogram(aes(x = velocidad, y = after_stat(density) ), fill = "red", alpha = 0.5, bins = 144)+
  scale_x_continuous(name = "Velocidad promedio (km/h)") +
  scale_y_continuous(name = "Densidad") +
  theme(
    legend.position = 'none'
  ) +
  geom_vline(xintercept = velocidad_media, color = "black", linetype = "dashed", size = 1) +
  annotate("text", x = velocidad_media + 5, y = 0.02, label = paste("Velocidad promedio:", round(velocidad_media, 2), "km/h"), size = 3) +
  geom_vline(xintercept = velocidad_media + velocidad_desvio, color = "black", linetype = "dashed", size = 1) +
  geom_vline(xintercept = velocidad_media - velocidad_desvio, color = "black", linetype = "dashed", size = 1)
```

Se puede observar que la distribucion de la velocidad es normal, con una media de 31.31 km/h y un desvio de 17.08 km/h.
El 68% de los datos se encuentran entre 14.23 km/h y 48.39 km/h.

### Volumen

Resumen de la variable volumen

minimo, maximo, promedio, desvio, cuartiles

```{r Resumen de la variable volumen, echo = FALSE}
(
    resumen_volumen <- load_data("resumen_volumen.csv",
    con,
      "
        SELECT
          MIN(volume) AS minimo,
          PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY volume) AS cuartil_1,
          PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY volume) AS mediana,
          PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY volume) AS cuartil_3,
          MAX(volume) AS maximo,
          AVG(volume) AS promedio,
          STDDEV(volume) AS desvio
        FROM fct_registros
      "
    )
)
```




Se ve que la cantidad minima de vehiculos registrados en 5 minutos es 0 y la maxima es 659.
El promedio de volumen es 17.28 vehiculos y el desvio estandar es 19.18 vehiculos.

Ahora veamos la distribucion del volumen registrado
Para mejor visualizacion, voy a dejar de lado los datos donde el volumen es 0

```{r, Distribucion de volumen Barras, fig.cap= "Distribucion de volumen"}
volumen_media <- mean(velocidad_volumen$volumen)
volumen_desvio <- sd(velocidad_volumen$volumen)

velocidad_volumen %>% ggplot() +
  geom_density(aes(x = volumen), fill = "blue", alpha = 0.5, kernel = "epanechnikov") +
  geom_histogram(aes(x = volumen, y = after_stat(density) ), fill = "red", alpha = 0.5, bins = 144)+
  scale_x_continuous(name = "Volumen de vehiculos") +
  scale_y_continuous(name = "Densidad") +
  theme(
    legend.position = 'none'
  ) +
  geom_vline(xintercept = volumen_media, color = "black", linetype = "dashed", size = 1) +
  annotate("text", x = volumen_media + 5, y = 0.02, label = paste("Volumen promedio:", round(volumen_media, 2), "vehiculos"), size = 3) +
  geom_vline(xintercept = volumen_media + volumen_desvio, color = "black", linetype = "dashed", size = 1) +
  geom_vline(xintercept = volumen_media - volumen_desvio, color = "black", linetype = "dashed", size = 1)
```

Se puede observar que la mayoria de los valores son menores a 100, en particular
el 75% de los datos son menores a `r round(resumen_volumen$cuartil_3, 2)` vehiculos.


# Preguntas de Investigacion

Las preguntas que dirigiran este analisis son las siguientes:
1.  ¿Existe alguna correlación entre el volumen y la velocidad?
2.  ¿Cuáles son las calles con los mayores promedios de velocidad en Montevideo? ¿Con que frecuencia se cometen excesos de velocidad?
3. ¿Cómo va variando el volumen y velocidad medidos a través del TIEMPO?


## 1. ¿Existe alguna correlación entre el volumen y la velocidad?

Haremos un grafico de puntos para visualizarlo.
Para los datos usaremos una muestra aleatoria de toda la base de datos 


```{r Muestra y Plot, fig.cap="Grafico de puntos de velocidad y volumen"}

velocidad_volumen_muestra <- velocidad_volumen %>% sample_n(nrow(velocidad_volumen)*0.3)

velocidad_volumen_muestra %>% ggplot() + geom_point(
  aes(
    x = velocidad,
    y= volumen
  )
)

```
Definitivamente **no hay una relacion lineal** entre velocidad y volumen

```{r Correlacion de velocidad y volumen}
cor(velocidad_volumen)
```
 La correlacion dio `r round(cor(velocidad_volumen$velocidad, velocidad_volumen$volumen), 2)`, lo que indica que no hay una correlacion lineal entre las variables.
 
 
## 2. ¿Cuáles son las calles con los mayores promedios de velocidad en Montevideo? ¿Con que frecuencia se cometen excesos de velocidad? 
 
Pasemos a investigar las calles con mayor promedio de velocidad.

```{r Importar datos de velocidad-calle, echo = FALSE }
velocidad_calles <- load_data( "velocidad_calles.csv",
  con,
  "
  SELECT d_sensores.dsc_avenida,
    AVG(velocidad) AS velocidad_promedio
  FROM fct_registros
  INNER JOIN d_sensores ON fct_registros.id_detector = d_sensores.id_detector
  GROUP BY dsc_avenida
  "
  )
```



```{r Calles con mayor promedio de velocidad, echo = FALSE, fig.cap="Calles con mayor promedio de velocidad"}
avenidas <- c("18 de Julio","8 de Octubre","Agraciada", "Av Brasil", "Av Italia","Bv Artigas","Bv Batlle y Ordonez","Bv Espana","C M Ramirez","Centenario","Fernandez Crespo","Bolivia","Albo","Belloni","Benito Blanco","Burges","Garibaldi","Garzon","Gral Flores","Millan","Morquio","Ponce","Rambla","Ricaldoni","Rivera","Rodo","San Martin","Saravia", "Soca","Uruguay","Varela", "L A de Herrera")  

velocidad_calles %>% 
  mutate(
    Tipo = ifelse(dsc_avenida %in% avenidas,
                  "Avenida", "Calle")
    ) %>% 
  arrange(desc(velocidad_promedio)) %>% 
  slice_head(n = 10) %>% 
  ggplot() +
  geom_col(
    aes(x = velocidad_promedio,
        y = reorder(dsc_avenida, velocidad_promedio),
        fill = Tipo) 
    ) +
  geom_text(
    aes( 
      x = velocidad_promedio, 
      y = dsc_avenida, 
      label = round(velocidad_promedio, 2)
    ),
    size = 5,
    nudge_x = 4,
  ) +
  labs(x = "Velocidad promedio",
       y = "Calle") +
  scale_fill_manual(
    values = c("Avenida" = "blue",
               "Calle" = "grey33")
    ) +
  theme_bw()
  
```

El siguiente gráfico nos muestra que las 3 calles con mas velocidad en promedio superan el máximo de 45km/h siendo este la velocidad máxima de circulación reglamentaria. La avenida más rápida en promedio no alcanza el máximo de velocidad perimitido

En promedio de velocidad circulacion los conductores son prudentes, aun asi vemos con que frecuencia se comenten excesos de velocidad.


```{r Importar datos de exceso de velocidad, echo=FALSE }
excesos <- load_data( "excesos.csv",
  con,
  "
  SELECT d_sensores.dsc_avenida,
    fct_registros.velocidad 
  FROM fct_registros TABLESAMPLE SYSTEM (10)
  JOIN d_sensores ON fct_registros.id_detector = d_sensores.id_detector
  "
  ) %>% 
  mutate(
    Tipo = ifelse(dsc_avenida %in% avenidas,
                  "Avenida", "Calle"),
    limite = ifelse(dsc_avenida %in% c("Bv Artigas",
                                       "Rambla",
                                       "Larranaga",
                                       "Bv Batlle y Ordonez",
                                       "Garzon"), 60, 45)
  )
```


```{r Grafico de , echo=FALSE, fig.cap = "Es netamente nula la proporcion de excesos de velocidad, podemos decir el los conductores en general respetan los limites de velocidad"}
excesos %>% 
  filter(velocidad > limite) %>% 
  group_by(dsc_avenida) %>%  
  summarise(
    cant = (n()/nrow(excesos))*100
    ) %>%
  arrange(desc(cant)) %>% 
  slice_head(n = 10) %>% 
  ggplot() +
  geom_col(
    aes(
      x = cant, 
      y = reorder(dsc_avenida, cant))
    ) + 
  geom_text(
    aes( 
      x = cant, 
      y = dsc_avenida, 
      label = paste(round(cant, 2),"%")
      ),
    size = 5,
    nudge_x = 0.15
  ) +
  labs(x = "Proporcion",
       y = "Calle") +
  theme_bw()
```

## 3. ¿Cómo va variando el volumen y velocidad medidos a traves de el tiempo?

```{r registros_maximos_hora, echo = FALSE}
registros_maximos_hora <- load_data("registros_maximos_hora.csv",
    con,
    "
      SELECT
        d_date.day_of_week,
        d_date.day_name,
        d_date.month_actual,
        d_date.month_name,
        d_date.year_actual,
        d_date.weekend_indr,
        CASE
          WHEN fct_registros.id_hora >= 0 AND fct_registros.id_hora < 100 THEN '00:00'
          WHEN fct_registros.id_hora >= 100 AND fct_registros.id_hora < 200 THEN '01:00'
          WHEN fct_registros.id_hora >= 200 AND fct_registros.id_hora < 300 THEN '02:00'
          WHEN fct_registros.id_hora >= 300 AND fct_registros.id_hora < 400 THEN '03:00'
          WHEN fct_registros.id_hora >= 400 AND fct_registros.id_hora < 500 THEN '04:00'
          WHEN fct_registros.id_hora >= 500 AND fct_registros.id_hora < 600 THEN '05:00'
          WHEN fct_registros.id_hora >= 600 AND fct_registros.id_hora < 700 THEN '06:00'
          WHEN fct_registros.id_hora >= 700 AND fct_registros.id_hora < 800 THEN '07:00'
          WHEN fct_registros.id_hora >= 800 AND fct_registros.id_hora < 900 THEN '08:00'
          WHEN fct_registros.id_hora >= 900 AND fct_registros.id_hora < 1000 THEN '09:00'
          WHEN fct_registros.id_hora >= 1000 AND fct_registros.id_hora < 1100 THEN '10:00'
          WHEN fct_registros.id_hora >= 1100 AND fct_registros.id_hora < 1200 THEN '11:00'
          WHEN fct_registros.id_hora >= 1200 AND fct_registros.id_hora < 1300 THEN '12:00'
          WHEN fct_registros.id_hora >= 1300 AND fct_registros.id_hora < 1400 THEN '13:00'
          WHEN fct_registros.id_hora >= 1400 AND fct_registros.id_hora < 1500 THEN '14:00'
          WHEN fct_registros.id_hora >= 1500 AND fct_registros.id_hora < 1600 THEN '15:00'
          WHEN fct_registros.id_hora >= 1600 AND fct_registros.id_hora < 1700 THEN '16:00'
          WHEN fct_registros.id_hora >= 1700 AND fct_registros.id_hora < 1800 THEN '17:00'
          WHEN fct_registros.id_hora >= 1800 AND fct_registros.id_hora < 1900 THEN '18:00'
          WHEN fct_registros.id_hora >= 1900 AND fct_registros.id_hora < 2000 THEN '19:00'
          WHEN fct_registros.id_hora >= 2000 AND fct_registros.id_hora < 2100 THEN '20:00'
          WHEN fct_registros.id_hora >= 2100 AND fct_registros.id_hora < 2200 THEN '21:00'
          WHEN fct_registros.id_hora >= 2200 AND fct_registros.id_hora < 2300 THEN '22:00'
          WHEN fct_registros.id_hora >= 2300 AND fct_registros.id_hora <= 2359 THEN '23:00'
        ELSE 'Unknown'
      END AS hora_rango,
        MAX(fct_registros.velocidad) AS max_velocidad,
        MAX(fct_registros.volume) AS max_volumen,
        AVG(fct_registros.velocidad) AS promedio_velocidad,
        AVG(fct_registros.volume) AS promedio_volumen,
        MAX(fct_registros.volumen_hora) as max_volumen_hora,
        AVG(fct_registros.volumen_hora) as promedio_volumen_hora,
        COUNT(fct_registros.velocidad) AS cant_registros
    FROM fct_registros
    INNER JOIN d_sensores ON fct_registros.id_detector = d_sensores.id_detector
    LEFT JOIN d_date ON fct_registros.id_fecha = d_date.id_fecha
    GROUP BY d_date.day_of_week, hora_rango, d_date.month_actual, d_date.year_actual, d_date.weekend_indr, d_date.day_name, d_date.month_name
    "
  )
```

```{r Datos para mapa de calor}
max_velocidad_hora_barrio <- load_data("max_velocidad_hora_barrio.csv",
    con,
    "
    SELECT
      d_sensores.barrio,
      MAX(fct_registros.velocidad) AS max_velocidad,
      AVG(fct_registros.velocidad) AS avg_velocidad,
      MAX(fct_registros.volume) AS max_volumen,
      AVG(fct_registros.volume) AS avg_volumen,
       CASE
        WHEN fct_registros.id_hora >= 0 AND fct_registros.id_hora < 100 THEN '00:00'
        WHEN fct_registros.id_hora >= 100 AND fct_registros.id_hora < 200 THEN '01:00'
        WHEN fct_registros.id_hora >= 200 AND fct_registros.id_hora < 300 THEN '02:00'
        WHEN fct_registros.id_hora >= 300 AND fct_registros.id_hora < 400 THEN '03:00'
        WHEN fct_registros.id_hora >= 400 AND fct_registros.id_hora < 500 THEN '04:00'
        WHEN fct_registros.id_hora >= 500 AND fct_registros.id_hora < 600 THEN '05:00'
        WHEN fct_registros.id_hora >= 600 AND fct_registros.id_hora < 700 THEN '06:00'
        WHEN fct_registros.id_hora >= 700 AND fct_registros.id_hora < 800 THEN '07:00'
        WHEN fct_registros.id_hora >= 800 AND fct_registros.id_hora < 900 THEN '08:00'
        WHEN fct_registros.id_hora >= 900 AND fct_registros.id_hora < 1000 THEN '09:00'
        WHEN fct_registros.id_hora >= 1000 AND fct_registros.id_hora < 1100 THEN '10:00'
        WHEN fct_registros.id_hora >= 1100 AND fct_registros.id_hora < 1200 THEN '11:00'
        WHEN fct_registros.id_hora >= 1200 AND fct_registros.id_hora < 1300 THEN '12:00'
        WHEN fct_registros.id_hora >= 1300 AND fct_registros.id_hora < 1400 THEN '13:00'
        WHEN fct_registros.id_hora >= 1400 AND fct_registros.id_hora < 1500 THEN '14:00'
        WHEN fct_registros.id_hora >= 1500 AND fct_registros.id_hora < 1600 THEN '15:00'
        WHEN fct_registros.id_hora >= 1600 AND fct_registros.id_hora < 1700 THEN '16:00'
        WHEN fct_registros.id_hora >= 1700 AND fct_registros.id_hora < 1800 THEN '17:00'
        WHEN fct_registros.id_hora >= 1800 AND fct_registros.id_hora < 1900 THEN '18:00'
        WHEN fct_registros.id_hora >= 1900 AND fct_registros.id_hora < 2000 THEN '19:00'
        WHEN fct_registros.id_hora >= 2000 AND fct_registros.id_hora < 2100 THEN '20:00'
        WHEN fct_registros.id_hora >= 2100 AND fct_registros.id_hora < 2200 THEN '21:00'
        WHEN fct_registros.id_hora >= 2200 AND fct_registros.id_hora < 2300 THEN '22:00'
        WHEN fct_registros.id_hora >= 2300 AND fct_registros.id_hora <= 2359 THEN '23:00'
        ELSE 'Unknown'
      END AS hora_rango
    FROM fct_registros
    INNER JOIN d_sensores ON
      fct_registros.id_detector = d_sensores.id_detector
    LEFT JOIN d_date ON
      fct_registros.id_fecha = d_date.id_fecha
    GROUP BY d_sensores.barrio, hora_rango
    "
  ) 
```

```{r}
tabla <- load_data( "tabla.csv",
  con,
  "
  SELECT
    fct_registros.volume, 
    fct_registros.velocidad,
    d_date.day_of_week
  FROM
    fct_registros TABLESAMPLE SYSTEM (0.5)
  INNER JOIN 
    d_sensores ON fct_registros.id_detector = d_sensores.id_detector
  INNER JOIN 
    d_date ON fct_registros.id_fecha = d_date.id_fecha
  WHERE
    fct_registros.velocidad <> 0 AND fct_registros.volume <> 0

  "
) %>% 
  dplyr::mutate( 
    dia_de_la_semana = 
      factor(
        day_of_week,
        levels = c(1, 2, 3, 4, 5, 6, 7),
        labels = c("Lunes", "Martes", "Miercoles", "Jueves", "Viernes", "Sabado", "Domingo")
      )
  )
```

```{r promedios_semanales, echo=FALSE}
promedios_semanales <- load_data("promedios_semanales.csv",
  con,
  "
  WITH tabla as (
    SELECT
      d_date.day_of_week as dia,
      fct_registros.velocidad as velocidad,
      fct_registros.volume as volumen
    FROM fct_registros
    LEFT JOIN d_date ON fct_registros.id_fecha = d_date.id_fecha
    )
    
    
    SELECT
      avg(velocidad) as avg_velocidad,
      avg(volumen) as avg_volumen,
      dia
    FROM tabla 
    GROUP BY dia
    "
  ) %>%
  mutate( 
    dia_semana = 
      factor(
        dia,
        levels = c(1, 2, 3, 4, 5, 6, 7),
        labels = c("Lunes", "Martes", "Miercoles",
                   "Jueves", "Viernes", "Sabado", "Domingo")
        ) 
    )
```

### Volumen
Primero observemos como va variado volumen y volumen_hora

#### Variacion por hora del dia

```{r Mapa de calor, fig.cap="Mapa de calor de volumen maxima por barrio por rango de hora"}
max_velocidad_hora_barrio %>%
  ggplot(
    aes(
      x = barrio,
      y = hora_rango,
      fill = avg_volumen
    )
  ) +
  geom_tile() +
  coord_fixed() +
  coord_flip() +
  scale_x_discrete( name = "Barrio") +
  scale_y_discrete( name = "Hora") +
  scale_fill_continuous(
    name =  "AVG Volumen",
    type = "viridis"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
    axis.text.y = element_text(size = 5)
  )


```


En Parque Rodo hay valores altos durante la noche, que lo diferencia del resto de los barrios.


#### Variacion por dia de la semana




```{r PLOT promedio volumen x dia de la semana, echo=FALSE, fig.cap="Promedio de variable Volumen por Dia de la semana}
promedios_semanales %>% ggplot() +
  geom_col(
    aes(
      x = dia_semana,
      y = avg_volumen,
    )
  ) + 
  geom_text(
    aes( 
      x = dia_semana, 
      y = avg_volumen, 
      label = round(avg_volumen, 2) 
    ),
    vjust = -0.5
  ) +
  labs(
    x = "Dia de la semana",
    y = "Promedio"
  ) 
```



```{r, echo = FALSE, fig.cap="Densidad de Volumen"}
tabla %>%
  ggplot() + 
  geom_density(
    aes(x = volume),
    kernel = "epanechnikov"
  ) +
  facet_wrap(~dia_de_la_semana) +
  scale_x_continuous(
    name = "Volumen",
    breaks = seq(0,100,10)
  ) +
  scale_y_continuous(
    name= "Cantidad"
  ) +
  coord_cartesian(xlim = c(0, 100))
```




#### Variacion por mes del año


#### Variacion por año






### Velocidad
Ahora observemos como va variando velocidad


#### Variacion por hora del dia

#### Variacion por dia de la semana

```{r PLOT promedio velocidad x dia de la semana, echo=FALSE, fig.cap="Promedio de variable Velocidad por Dia de la semana}
promedios_semanales %>% ggplot() +
  geom_col(
    aes(
      x = dia_semana,
      y = avg_velocidad,
    )
  ) + 
  geom_text(
    aes( 
      x = dia_semana, 
      y = avg_velocidad, 
      label = round(avg_velocidad, 2) 
    ),
    vjust = -0.5
  ) +
  labs(
    x = "Dia de la semana",
    y = "Promedio"
  ) 
```

```{r, echo = FALSE, fig.cap="Densidad de Velocidad"}
tabla %>%
  ggplot() + 
  geom_density(
    aes(x = velocidad),
    kernel = "epanechnikov"
  ) +
  facet_wrap(~dia_de_la_semana) + 
  scale_x_continuous(breaks = seq(0, 150, 20),
                     name = "Velocidad"
  ) +
  scale_y_continuous(
    name= "Densidad"
  )
```

```{r Grafico de distribucion max_velocidad por hora, echo = FALSE}
mediana <- registros_maximos_hora %>% 
  summarise(mediana = median(max_velocidad))
mediana <- as.numeric(mediana)

my_labeller <- function(x) {
  ifelse(x, "Fin de semana", "Entre semana")
}

registros_maximos_hora %>%
  ggplot(
    aes(
      x = max_velocidad,
      y = fct_rev(hora_rango)
    )
  ) +
  geom_boxplot(
    aes(
      fill=hora_rango
    )
  ) +
  stat_summary(fun = mean, geom = "point", size = 3, alpha = 0.6) +
  geom_vline(xintercept = mediana) +
  facet_grid(
    ~weekend_indr,
    labeller = as_labeller(my_labeller)
  )+
  theme(
    legend.position = "none"
  ) +
  scale_x_continuous(
    name = "Velocidad Maxima Registrada"
  ) +
  scale_y_discrete(
    name= "Hora del día"
  )

```
Entre las 00:00 y las 08:00 es donde se alcanzan picos de casi 140 km/h, luego de ese rango se reduce la tendencia por ser horario laboral para luego crecer a medida de que cae la noche.
Durante el fin de semana las velocidades máximas durante el horario laboral de lunes a viernes es totalmente diferente, los conductores durante el fin de semana aprovechan la baja de volumen para circular mas rápido.



```{r Densidades de max_velocidad x mes del año}
registros_maximos_hora %>%
  ggplot(
    aes(
      x =  max_velocidad,
      fill = weekend_indr # esto añade el color según el factor esFinDeSemana
    )
  ) +
  geom_density(
    alpha = 0.5,
    kernel = "epanechnikov"
  ) +
  facet_wrap(~month_actual)+
  scale_x_continuous(
    name = "Velocidad Maxima Registrada"
  )+
  scale_y_continuous(
    name = "Densidad"
  ) +
  scale_fill_discrete(
    name = "Dias de la semana",
    labels = c("Entre semana", "Fin de semana")
  )
  

```



#### Variacion por mes del año


#### Variacion por año

Si entramos en cada año vemos como crece la densidad de velocidad máxima registrada de lunes a viernes, 
```{r, echo = FALSE, fig.cap = '2023 no esta completo, vamos a profundizar mas en nuestro modelo predictivo'}
registros_maximos_hora %>%
  ggplot(
    aes(
      x =  max_velocidad,
      fill = weekend_indr
    )
  ) +
  geom_density(
    alpha = 0.5,
    kernel = "epanechnikov"
  ) +
  facet_grid(~year_actual) +
  theme(legend.position = "bottom")+
  scale_x_continuous(
    name = "Velocidad Maxima Registrada"
  )+
  scale_y_continuous(
    name = "Densidad"
  ) +
  scale_fill_discrete(
    name = "Dias de la semana",
    labels = c("Entre semana", "Fin de semana")
  )
  
```

Ahora la densidad de velocidad durante los dias siempre se encuentran en un rango de (0,60]


### Variacion conjunta de velocidad y volumen






En el gráfico se puede observar: - La velocidad media se mantiene casi constante durante toda la semana, salvo una leve variación positiva los fines de semana. - El volumen medio de vehículos detectados por los radares va variando en la semana, alcanzado su pico los viernes. También es considerablemente inferior los fines de semana.


Si bien previamente obsevamos leves exceos de velocidad, dentro de esos excesos se registran velocidades maximas de mas de 100km/h, veamos esos maximos pro franja horaria segun si fue registrado en un fin de semana o no










Con el volumen de trafico, hay una gran diferencia el domingo comparado con el resto de los dias, en todos los dias la mayoria de volumen se concentra en el rango (0,10] donde en dicho rango alcanza su pico de circulacion en todo Montevideo.






## Resultados interesantes

[Presentar los resultados más relevantes obtenidos durante el análisis exploratorio.]


# Modelo estadístico
Para el diseño del modelo, nos pareció interesante evaluar la interacción entre el volumen y la velocidad, además de otros factores planteados en las preguntas iniciales, como la hora o el día de la semana. 
Para esto, observamos que estas variables están claramente correlacionadas, por lo cual no es viable hacer un modelo de regresión ya que es necesaria la independencia de los errores. Por esto, concluimos en que es una mejor opción hacer un arbol de decisión, ya que nos permite observar esta dependencia con más claridad.


## Predicir velocidad promedio de un sensor

Ya definido el tipo de modelo, nos resta definir la variable de respuesta y sus predictoras.
Nos pareció que la mejor opción para ser variable de respuesta era la velocidad, ya que vemos que cada uno de los otros factores son condicionantes para esta variable. 

Luego, nuestras variables predictoras serán el volumen, la hora y el día de la semana.

Para la hora y el día, decidimos seccionar las variables de forma binaria, ya que observamos en la velocidad una tendencia de comportamiento distinta en dos bloques bien definidos de cada variable. 
La hora estará seccionada en "día" y "noche" siendo "día" entre las 8:00 y las 20:00, y "noche" el caso contrario. 
Para el día de la semana, seccionaremos los datos en "fin de semana" y "día de semana", ya que el tráfico suele comportarse de maneras diferentes en cada caso.

Para el volumen y la velocidad, tomamos el promedio para simplificar los datos.
Los datos están agrupados por detector, hora, y día de la semana (Fin de semana o no).




```{r Datos para arbol}
datos_arbol <- load_data("datos_arbol.csv",
  con,
  "
  SELECT
    AVG(fct_registros.volume) as avg_volumen,
    AVG(fct_registros.velocidad) as avg_velocidad,
    d_date.weekend_indr as esFinDeSemana,
    CASE
            WHEN fct_registros.id_hora >= 600 AND fct_registros.id_hora < 1900 THEN TRUE
            ELSE FALSE
    END AS esDeDia
FROM fct_registros
LEFT JOIN d_date ON fct_registros.id_fecha = d_date.id_fecha
GROUP BY
    fct_registros.id_detector,
    esFinDeSemana,
    esDeDia
  "
)

nrow(datos_arbol)
```
Ya tenemos los datos, nos queda armar el arbol.
Primero, tomaremos una muestra para crear los conjuntos de entrenamiento y prueba. La proporción será de un 70% para entrenamiento y un 30% para prueba. Antes de tomar la muestra fijaremos una semilla para poder analizar el mismo modelo de forma reproducible.

```{r Arbol training}
set.seed(946)
intrain <- sample(x = 1:nrow(datos_arbol), size = nrow(datos_arbol)*0.7)

training <- datos_arbol[intrain,]

testing <- datos_arbol[-intrain,]

arbol_vol <- rpart(avg_velocidad ~ esfindesemana + esdedia + avg_volumen, data = training)
```


Al observar el modelo, notaremos que el volumen aparece repetidas veces seccionando los datos. Esto es porque, al ser continua, hay una variabilidad mucho más alta, y hay más casos para observar.
```{r Arbol Plot}
rpart.plot(arbol_vol, digits = (-4))

```
En concordancia con el comentario anterior, la variable volumen divide los datos en 4 categorías (Con límites 2.01; 3.93; 10.11) y la variable "esdedia" solo actúa una vez, mientras la del fin de semana ni siquiera es utilizada por el arbol.

Poodemos observar que los datos donde el volumen es menor a 3.93 es una minoría, ya que abarcan poco más de una décima parte. De todas formas, hay una división marcada en estos datos, ya que si el volumen es menor a 2, la velocidad suele ser de 6km/h, mientras que cuando el volumen es mayor a 2, la velocidad aumenta a casi 21km/h (Además es más significativa la cantidad de datos).

Cuando el volumen es mayor a 3.93 también hay un límite que demarca un comportamiento distinto entre los datos que superan y no este número, y es el 10.11. 
Los datos que tienen un volumen menor a 10.11, abarcan casi un tercio de los datos con una velocidad promedio de 28.29km/h, y los datos con volumen mayor a 10.11 abarcan casi un 58% de los datos con una velocidad promedio de 33 km/h. Se observa que la velocidad es mayor cuando el volumen es mayor a 10.11, aunque no es una diferencia tan significativa.

Una observación interesante es la diferencia de comportamiento entre los datos del día y de noche según el volumen. En todos los casos la velocidad es considerablemente mayor de noche que de día, pero la cantidad de datos observados es mayor de noche si el volumen es menor a 10, pero es mayor de día si el volumen es mayor a 10. Es decir, cuando el volumen es menor, hay más observaciones de noche, pero cuando hay mucho volumen de tráfico, hay menos observaciones de noche y de día hay muchas más. 




Para saber la fiabilidad del modelo, calculamos el error cuadrado medio, y en base a eso fuimos ajustando los parámetros del modelo hasta llegar al actual, ya que es el que menor error tiene.
```{r Arbol testing}
rmse(arbol_vol, training)
rmse(arbol_vol, testing)
```
El modelo se aleja alrededor de esa cantidad de kilometros de los datos reales. Hay un mayor error en el conjunto de prueba, ya que son menos valores.


## Random Forest


```{r Datos Random Forest}
forest_data <- load_data("forest_data.csv",
  con,
  "
  SELECT
    AVG(fct_registros.volume) as avg_volumen,
    AVG(fct_registros.velocidad) as avg_velocidad,
    MAX(fct_registros.volume) as max_volumen,
    fct_registros.id_carril,
    fct_registros.id_hora
FROM fct_registros 
LEFT JOIN d_date ON fct_registros.id_fecha = d_date.id_fecha
GROUP BY
    fct_registros.id_detector,
    fct_registros.id_carril,
    fct_registros.id_hora
  "
)

nrow(forest_data)
```


```{r Random Forest training, eval=FALSE}
intrain <- sample(x = 1:nrow(forest_data), size = nrow(forest_data)*0.7)

training <- forest_data[intrain,]
testing <- forest_data[-intrain,]

rf_avg_velocidad <- randomForest(avg_velocidad~ .,data= training)
```


```{r Random Forest testing, eval=FALSE}
#raiz cuadrada del error cuadratico medio
rmse(rf_avg_velocidad, testing)
```
El modelo le erra por esa cantidad de kilometros/hora en promedio

```{r Random Forest Plot, eval=FALSE}
varImpPlot(rf_avg_velocidad)
```
Este grafico muestra que variables son mas significativas para la prediccion del promedio de velocidad

## Predicciones

[Presentar las predicciones realizadas por el modelo.]

## Interpretación de resultados

[Interpretar los resultados obtenidos del modelo estadístico.]

# Aplicación Shiny

## Descripción

La aplicación consta de 4 partes. Un mapa con la ubicación de los semáforos, un analisis univariado, otro multivariado y el modelo.
La palicacion resume todo lo hecho anteriormente.
[Enlace](https://ivan1arriola.shinyapps.io/tareaFinal/)


# Comentarios finales

## Hallazgos principales

[Resumir los principales hallazgos del proyecto.]

## Posibles extensiones

[Discutir posibles extensiones o mejoras para el proyecto.]


# Referencias

 
 